{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c4c46c",
   "metadata": {},
   "source": [
    "### **Step 1**: Import configs and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6178d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from backend.data_processing_simple import process_dataset_simple, print_dataframe_sample, load_and_filter_data, flatten_passages, add_negative_samples, convert_to_triplets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "# Load config\n",
    "with open('../backend/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Dataset paths\n",
    "datasets = {\n",
    "    'train': '../data/ms_marco_train.parquet',\n",
    "    'validation': '../data/ms_marco_validation.parquet', \n",
    "    'test': '../data/ms_marco_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf147174",
   "metadata": {},
   "source": [
    "### **Step 2**: Process data to get triplets for train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ca90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä SAMPLING CONFIGURATION:\n",
      "  Total samples to process: 100,000\n",
      "  Train split: 70% (70,000 samples)\n",
      "  Test split: 20% (20,000 samples)\n",
      "  Validation split: 10% (10,000 samples)\n",
      "\n",
      "Processing datasets to triplet format...\n",
      "==================================================\n",
      "üìÅ Processing MS_MARCO_TRAIN.PARQUET dataset (target: 70,000 samples)...\n",
      "üìÇ Loading data from: ../data/ms_marco_train.parquet\n",
      "  ‚ö†Ô∏è Standard pandas read failed: Nested data conversions not implemented for chunked array outputs\n",
      "  üîß Trying alternative approach for nested data...\n",
      "    ‚úÖ Successfully loaded using fastparquet engine\n",
      "  Loaded: 808,731 samples\n",
      "  ‚úÇÔ∏è Sampled down to: 70,000 samples\n",
      "\n",
      "============================================================\n",
      "RAW DATA LOADED\n",
      "Shape: (70000, 8)\n",
      "Columns: ['answers', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'passages.is_selected', 'passages.passage_text', 'passages.url']\n",
      "Sample data (first 2 rows):\n",
      "                                                                                                    answers                                                                   query  query_id   query_type wellFormedAnswers            passages.is_selected                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             passages.passage_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    passages.url\n",
      "19000                                                                                  [No Answer Present.]  what happened to money multiplier if the reserve banking is under 100%    665215  DESCRIPTION                []  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  [fractional-reserve banking but not under 100-percent-reserve banking.____ 2. If the reserve ratio is 12.5 percent, the money multiplier isa. 6.25.b. 8.c. 12.5.d. 25.____ 3. If the reserve ratio is 100 percent, then a new deposit of $500 into a bank accounta., The money multiplier (also called the credit multiplier or the deposit multiplier) is a measure of the extent to which the creation of money in the banking system causes the growth in the money supply to exceed growth in the monetary base., Banks and money supply 6. 100% reserve banking and Fractional reserve banking and reserve ratio 7.The Money multiplier 8. Bank capital, leverage, and financial crisis 9.The Fed's Tools of Monetary Control (influence on quantity of reserves, reserve ratio, and problems controlling the money supply?, Money Supply and the Money Multiplier. Money, either in the form of currency or as bank reserves, is a liability of the central bank. The central bank controls the monetary base, expanding or contracting it at will, according to the needs of the economy., The existence of the money multiplier is the outcome of fractional reserve banking, which the current banking system makes possible. In short, it is the existence of the central bank that enables banks to practice fractional reserve banking, thereby creating inflationary credit., (1 + c)/(r + e + c). The money multiplier (also called the credit multiplier or the deposit multiplier) is a measure of the extent to which the creation of money in the banking system causes the growth in the money supply to exceed growth in the monetary base., Fractional reserve banking is a system where banks hold less than 100% of their deposits as reserves.Let r = fraction of deposits held as reserves.Then bank loans out $ (1-r)*D. So, now, D is still $1000. But, C = $800 (borrower who got the loan now has $800 in currency)., Money in the US economy 3. The Federal Reserve system 4. The Federal Open Market Committee 5. Banks and money supply 6. 100% reserve banking and Fractional reserve banking and reserve ratio 7.The Money multiplier 8., So the idea is that the banks would have a 100 per cent reserve-deposit ratio which ensures that the money supply is 1-for-1 multiple of the monetary base. Any firm that wanted to borrow funds would have to issue a convincing asset as collateral., 2. 100% reserve banking Money supply (M) = sum of currency (C) + demand deposits (D)C = currency (cash) held by the public and currency held by banksD = deposits at banks which the public can withdraw on demand (e.g., checking accounts) Imagine a world with no banks.]  [http://www.shsu.edu/~eco_hkn/classes/ECO23405_Quiz8_Spring10.pdf, http://moneyterms.co.uk/money-multiplier/, https://quizlet.com/11259441/macro-test-2-chapter-16-the-monetary-system-flash-cards/, http://thismatter.com/money/banking/money-supply-money-multiplier.htm, https://mises.org/library/money-multiplier-myth-or-reality, http://moneyterms.co.uk/money-multiplier/, http://www2.econ.iastate.edu/faculty/bhattacharya/102H/money%20supply.pdf, https://quizlet.com/11259441/macro-test-2-chapter-16-the-monetary-system-flash-cards/, http://bilbo.economicoutlook.net/blog/?p=7299, http://www2.econ.iastate.edu/faculty/bhattacharya/102H/money%20supply.pdf]\n",
      "534732  [The first bloody violent video game is The Texas Chainsaw Massacre video game for the atari 2600.]                                   what was the first violent video game    920646       ENTITY                []  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]                     [However, the incident prompted a wave of legislative and bureaucratic efforts against violent video games in the following months, including a meeting between US vice president, Joe Biden, and representatives from the video game industry on the topic of video game violence., First Violent Video Game. i am a gamer and am not violent you can go on line and they will blame things on video games i have heard a lot of things like halo teaches people how to snipe. And i know that i play violent video games or fighting games with big combos to let off some steam., Best Answer: Well it depends on your definition of violent. Does Mario killing people by jumping on their heads count as violent? In any case, the first game that brought about any significant controversy would be the 1976 release Death Race. Rather than copy and paste a bunch of stuff from Wikipedia, read the below..., The first one would probably be death race 2000, but the first bloody violent video game would be The Texas Chainsaw Massacre video game for the atari 2600, which caused a surprisingly small amount of controversy. This Site Might Help You., Studies of violent video game playing and crime have generally not supported the existence of causal links. Evidence from studies of juveniles as well as criminal offenders has generally not uncovered evidence for links., What was the first violent video game? Im doing a contreversy paper on violent video games,need help please., Asker's rating. 1  First Violent Video Game. 2  i am a gamer and am not violent you can go on line and they will blame things on video games i have heard a lot of things like halo teaches people how to snipe. 3  For the best answers, search on this site https://shorturl.im/jHjmh., Results show that there were no significant effects of video game playing in the short term, with violent video games and non-violent video games having no significant differences, indicating that children do not have decreased empathy from playing violent video games., Some studies have examined the consumption of violent video games in society and violent crime rates. Generally, it is acknowledged that societal violent video game consumption has been associated with over an 80% reduction in youth violence in the US during the corresponding period., The positive and negative characteristics and effects of video games are the subject of scientific study. Results of investigations into links between video games and addiction, aggression, violence, social development, and a variety of stereotyping and sexual morality issues are debated.]                                    [https://en.wikipedia.org/wiki/Video_game_controversies, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://en.wikipedia.org/wiki/Video_game_controversies, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://en.wikipedia.org/wiki/Video_game_controversies, https://en.wikipedia.org/wiki/Video_game_controversies, https://en.wikipedia.org/wiki/Video_game_controversies]\n",
      "============================================================\n",
      "üîç Filtering valid data...\n",
      "  After filtering: 43,240 samples\n",
      "\n",
      "============================================================\n",
      "FILTERED DATA\n",
      "Shape: (43240, 8)\n",
      "Columns: ['answers', 'query', 'query_id', 'query_type', 'wellFormedAnswers', 'passages.is_selected', 'passages.passage_text', 'passages.url']\n",
      "Sample data (first 2 rows):\n",
      "                                                                                                    answers                                  query  query_id   query_type wellFormedAnswers            passages.is_selected                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      passages.passage_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  passages.url\n",
      "534732  [The first bloody violent video game is The Texas Chainsaw Massacre video game for the atari 2600.]  what was the first violent video game    920646       ENTITY                []  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]                                                                                                                                                                              [However, the incident prompted a wave of legislative and bureaucratic efforts against violent video games in the following months, including a meeting between US vice president, Joe Biden, and representatives from the video game industry on the topic of video game violence., First Violent Video Game. i am a gamer and am not violent you can go on line and they will blame things on video games i have heard a lot of things like halo teaches people how to snipe. And i know that i play violent video games or fighting games with big combos to let off some steam., Best Answer: Well it depends on your definition of violent. Does Mario killing people by jumping on their heads count as violent? In any case, the first game that brought about any significant controversy would be the 1976 release Death Race. Rather than copy and paste a bunch of stuff from Wikipedia, read the below..., The first one would probably be death race 2000, but the first bloody violent video game would be The Texas Chainsaw Massacre video game for the atari 2600, which caused a surprisingly small amount of controversy. This Site Might Help You., Studies of violent video game playing and crime have generally not supported the existence of causal links. Evidence from studies of juveniles as well as criminal offenders has generally not uncovered evidence for links., What was the first violent video game? Im doing a contreversy paper on violent video games,need help please., Asker's rating. 1  First Violent Video Game. 2  i am a gamer and am not violent you can go on line and they will blame things on video games i have heard a lot of things like halo teaches people how to snipe. 3  For the best answers, search on this site https://shorturl.im/jHjmh., Results show that there were no significant effects of video game playing in the short term, with violent video games and non-violent video games having no significant differences, indicating that children do not have decreased empathy from playing violent video games., Some studies have examined the consumption of violent video games in society and violent crime rates. Generally, it is acknowledged that societal violent video game consumption has been associated with over an 80% reduction in youth violence in the US during the corresponding period., The positive and negative characteristics and effects of video games are the subject of scientific study. Results of investigations into links between video games and addiction, aggression, violence, social development, and a variety of stereotyping and sexual morality issues are debated.]  [https://en.wikipedia.org/wiki/Video_game_controversies, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://en.wikipedia.org/wiki/Video_game_controversies, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://answers.yahoo.com/question/index?qid=20091114155239AAmXsN4, https://en.wikipedia.org/wiki/Video_game_controversies, https://en.wikipedia.org/wiki/Video_game_controversies, https://en.wikipedia.org/wiki/Video_game_controversies]\n",
      "792237                                                                [$1,443 for men and $1,042 for women]      what are personal weekly earnings    563174  DESCRIPTION                []  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]  [Personal income is an individual's total earnings from wages, investment interest, and other sources. The US Census Bureau reported a median personal income of $30,240 for all workers over age 15 with income, and a mean personal income of $44,510 based on the Current Population Survey for 2015., Income in the United Kingdom. In terms of global poverty criteria, the United Kingdom is a wealthy country, with virtually no people living on less than ¬£10 a day. In 2012-13, median income was approximately ¬£21,000 a year but varies considerably by age, location, data source and occupation., BEA produces monthly estimates of personal income for the nation, quarterly estimates of state personal income, and annual estimates of local-area personal income. More information is found on BEA's website., Workers age 16 to 24 had the lowest median weekly earnings, at $516. (See table 3.) --Among the major occupational groups, persons employed full time in management, professional, and related occupations had the highest median weekly earnings--$1,443 for men and $1,042 for women., Highlights from the fourth-quarter data: --Median weekly earnings of full-time workers were $849 in the fourth quarter of 2016. Women had median weekly earnings of $758, or 81.8 percent of the $927 median for men. (See table 2.) --The women's-to-men's earnings ratio varied by race and ethnicity., In the twelve months to November 2016, Full-Time Adult Average Weekly Ordinary Time Earnings increased by 2.2% to $1,533.10. The Full-Time Adult Average Weekly Total Earnings in November 2016 was $1,592.40, a rise of 2.2% from the same time last year., NOVEMBER KEY FIGURES. The following table contains the key Average Weekly Earnings figures for the November 2016 reference period. The Australian Bureau of Statistics' Average Weekly Earnings survey is designed to measure the level of average earnings in Australia at a point in time., (See table 2.) --Usual weekly earnings of full-time workers varied by age. Among men, those ages 55 to 64 and 45 to 54 had the highest median weekly earnings, at $1,113 and $1,102, respectively. For women, usual weekly earnings were highest for those ages 35 to 44 ($859) and 45 to 54 ($840)., The Survey of Personal Incomes (SPI) is a dataset from HM Revenue and Customs (HMRC) based on individuals who could be liable to tax. HMRC does not hold information on individuals whose income is below the personal allowance (¬£8,105 in 2012/13)., Inflation at 2.3%, pay increasing at 2.2%. For the 5th consecutive year, inflation was higher than the rise in median weekly earnings. The gap has narrowed however since 2011 when earnings crept up by just 0.4% and inflation was at 4.5%. From 1998 to 2009, increases in earnings were above inflation.]               [https://en.wikipedia.org/wiki/Personal_income_in_the_United_States, https://en.wikipedia.org/wiki/Income_in_the_United_Kingdom, https://en.wikipedia.org/wiki/Personal_income_in_the_United_States, https://www.bls.gov/news.release/wkyeng.nr0.htm, https://www.bls.gov/news.release/wkyeng.nr0.htm, http://www.abs.gov.au/ausstats/abs@.nsf/mf/6302.0, http://www.abs.gov.au/ausstats/abs@.nsf/mf/6302.0, https://www.bls.gov/news.release/wkyeng.nr0.htm, https://en.wikipedia.org/wiki/Income_in_the_United_Kingdom, https://www.theguardian.com/news/datablog/2013/dec/12/uk-median-weekly-pay-is-517-but-who-earns-that]\n",
      "============================================================\n",
      "  üîÑ Flattening data...\n",
      "\n",
      "üîç Example of nested data structure:\n",
      "Query: what was the first violent video game\n",
      "Query ID: 920646\n",
      "Passages: 10 passages\n",
      "First passage: However, the incident prompted a wave of legislative and bureaucratic efforts against violent video ...\n",
      "Is selected: [0, 0, 0, 1, 0]...\n",
      "  Flattened: 431,525 rows\n",
      "\n",
      "============================================================\n",
      "FLATTENED DATA\n",
      "Shape: (431525, 5)\n",
      "Columns: ['query', 'query_id', 'query_type', 'document', 'is_relevant']\n",
      "Sample data (first 3 rows):\n",
      "                                   query  query_id query_type                                                                                                                                                                                                                                                                                                                          document  is_relevant\n",
      "0  what was the first violent video game    920646     ENTITY                                                However, the incident prompted a wave of legislative and bureaucratic efforts against violent video games in the following months, including a meeting between US vice president, Joe Biden, and representatives from the video game industry on the topic of video game violence.            0\n",
      "1  what was the first violent video game    920646     ENTITY                                    First Violent Video Game. i am a gamer and am not violent you can go on line and they will blame things on video games i have heard a lot of things like halo teaches people how to snipe. And i know that i play violent video games or fighting games with big combos to let off some steam.            0\n",
      "2  what was the first violent video game    920646     ENTITY  Best Answer: Well it depends on your definition of violent. Does Mario killing people by jumping on their heads count as violent? In any case, the first game that brought about any significant controversy would be the 1976 release Death Race. Rather than copy and paste a bunch of stuff from Wikipedia, read the below...            0\n",
      "============================================================\n",
      "  ‚ûï Adding negative samples...\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED PROCESSING - Use this instead of the cell above\n",
    "# Add missing import for export_to_csv\n",
    "from backend.data_processing_simple import export_to_csv\n",
    "\n",
    "# Sampling configuration - modify these as needed\n",
    "print(f\"üìä SAMPLING CONFIGURATION:\")\n",
    "print(f\"  Total samples to process: {config['TOTAL_SAMPLES']:,}\")\n",
    "print(f\"  Train split: {config['TRAIN_SPLIT']*100:.0f}% ({int(config['TOTAL_SAMPLES']*config['TRAIN_SPLIT']):,} samples)\")\n",
    "print(f\"  Test split: {config['TEST_SPLIT']*100:.0f}% ({int(config['TOTAL_SAMPLES']*config['TEST_SPLIT']):,} samples)\")\n",
    "print(f\"  Validation split: {(1-config['TRAIN_SPLIT']-config['TEST_SPLIT'])*100:.0f}% ({int(config['TOTAL_SAMPLES']*(1-config['TRAIN_SPLIT']-config['TEST_SPLIT'])):,} samples)\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\nProcessing datasets to triplet format...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate samples per dataset\n",
    "samples_per_dataset = {\n",
    "    'train': int(config['TOTAL_SAMPLES'] * config['TRAIN_SPLIT']),\n",
    "    'test': int(config['TOTAL_SAMPLES'] * config['TEST_SPLIT']),\n",
    "    'validation': int(config['TOTAL_SAMPLES'] * (1 - config['TRAIN_SPLIT'] - config['TEST_SPLIT']))\n",
    "}\n",
    "\n",
    "# Process each dataset using simple processing\n",
    "for name, input_path in datasets.items():\n",
    "    target_samples = samples_per_dataset[name]\n",
    "    \n",
    "    # Use the simple processing pipeline that handles everything internally\n",
    "    triplets = process_dataset_simple(input_path, target_samples)\n",
    "    export_to_csv(triplets, f'../data/triplets_{name}.csv')\n",
    "    \n",
    "    # Store result\n",
    "    results[name] = triplets\n",
    "    print(f\"  ‚úÖ {name.upper()} dataset completed!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "total_triplets = 0\n",
    "for name, triplets_df in results.items():\n",
    "    triplets_count = len(triplets_df)\n",
    "    total_triplets += triplets_count\n",
    "    print(f\"{name.upper()}: {triplets_count:,} triplets, {triplets_df['query'].nunique():,} unique queries\")\n",
    "\n",
    "print(f\"\\nüéØ TOTAL TRIPLETS: {total_triplets:,}\")\n",
    "\n",
    "print(\"\\nüéØ Sample triplet from train dataset:\")\n",
    "if 'train' in results and len(results['train']) > 0:\n",
    "    sample = results['train'].iloc[0]\n",
    "    print(f\"Query: {sample['query'][:80]}...\")\n",
    "    print(f\"Positive: {sample['positive_example'][:80]}...\")\n",
    "    print(f\"Negative: {sample['negative_example'][:80]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ All datasets processed! Results stored in 'results' dictionary.\")\n",
    "print(\"Access with: results['train'], results['validation'], results['test']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sampling configuration - modify these as needed\n",
    "print(f\"üìä SAMPLING CONFIGURATION:\")\n",
    "print(f\"  Total samples to process: {config['TOTAL_SAMPLES']:,}\")\n",
    "print(f\"  Train split: {config['TRAIN_SPLIT']*100:.0f}% ({int(config['TOTAL_SAMPLES']*config['TRAIN_SPLIT']):,} samples)\")\n",
    "print(f\"  Test split: {config['TEST_SPLIT']*100:.0f}% ({int(config['TOTAL_SAMPLES']*config['TEST_SPLIT']):,} samples)\")\n",
    "print(f\"  Validation split: {(1-config['TRAIN_SPLIT']-config['TEST_SPLIT'])*100:.0f}% ({int(config['TOTAL_SAMPLES']*(1-config['TRAIN_SPLIT']-config['TEST_SPLIT'])):,} samples)\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\nProcessing datasets to triplet format...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate samples per dataset\n",
    "samples_per_dataset = {\n",
    "    'train': int(config['TOTAL_SAMPLES'] * config['TRAIN_SPLIT']),\n",
    "    'test': int(config['TOTAL_SAMPLES'] * config['TEST_SPLIT']),\n",
    "    'validation': int(config['TOTAL_SAMPLES'] * (1 - config['TRAIN_SPLIT'] - config['TEST_SPLIT']))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for name, input_path in datasets.items():\n",
    "    target_samples = samples_per_dataset[name]\n",
    "\n",
    "    df = process_dataset_simple(input_path)\n",
    "    triplets = convert_to_triplets(df)\n",
    "    export_to_csv(triplets, f'data/triplets_{name}.csv')\n",
    "    print(f\"  ‚úÖ {name.upper()} dataset completed!\")\n",
    "\n",
    "    # Store result\n",
    "    results[name] = triplets\n",
    "    print(f\"  ‚úÖ {name.upper()} datasetcompleted!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "total_triplets = 0\n",
    "for name, triplets_df in results.items():\n",
    "    triplets_count = len(triplets_df)\n",
    "    total_triplets += triplets_count\n",
    "    print(f\"{name.upper()}: {triplets_count:,} triplets, {triplets_df['query'].nunique():,} unique queries\")\n",
    "\n",
    "print(f\"\\nüéØ TOTAL TRIPLETS: {total_triplets:,}\")\n",
    "\n",
    "print(\"\\nüéØ Sample triplet from train dataset:\")\n",
    "if 'train' in results and len(results['train']) > 0:\n",
    "    sample = results['train'].iloc[0]\n",
    "    print(f\"Query: {sample['query'][:80]}...\")\n",
    "    print(f\"Positive: {sample['positive_example'][:80]}...\")\n",
    "    print(f\"Negative: {sample['negative_example'][:80]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ All datasets processed! Results stored in 'results' dictionary.\")\n",
    "print(\"Access with: results['train'], results['validation'], results['test']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44619a",
   "metadata": {},
   "source": [
    "### **Step 3**: Select sub-sample triplets for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.data_processing_simple import convert_to_training_format\n",
    "\n",
    "# Convert processed results to training format (no subsampling needed - already done!)\n",
    "train_data = convert_to_training_format(results['train'])\n",
    "val_data = convert_to_training_format(results['validation']) \n",
    "test_data = convert_to_training_format(results['test'])\n",
    "\n",
    "# Print sample to verify format\n",
    "print(\"Sample training triplet:\")\n",
    "print(f\"Query: {train_data[0][0][:100]}...\")\n",
    "print(f\"Positive: {train_data[0][1][:100]}...\")  \n",
    "print(f\"Negative: {train_data[0][2][:100]}...\")\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_data):,} triplets\")\n",
    "print(f\"  Validation: {len(val_data):,} triplets\")\n",
    "print(f\"  Test: {len(test_data):,} triplets\")\n",
    "\n",
    "# Use training data for the model\n",
    "data = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e92a1",
   "metadata": {},
   "source": [
    "### **Step 4**: Select sub-sample triplets for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d19900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenizer and Vocab ---\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "class PretrainedTokenizer:\n",
    "    def __init__(self, word_to_idx_path):\n",
    "        # Load pretrained word_to_idx mapping\n",
    "        with open(word_to_idx_path, 'rb') as f:\n",
    "            self.word2idx = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded vocabulary with {len(self.word2idx):,} tokens\")\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        # Only include words that exist in vocabulary, skip unknown words\n",
    "        return [self.word2idx[word.lower()] for word in sentence.split() if word.lower() in self.word2idx]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea44b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained tokenizer\n",
    "tokenizer = PretrainedTokenizer(config['WORD_TO_IDX_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ba67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset Class ---\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query, pos_doc, neg_doc = self.data[idx]\n",
    "        return (torch.tensor(self.tokenizer.encode(query), dtype=torch.long),\n",
    "                torch.tensor(self.tokenizer.encode(pos_doc), dtype=torch.long),\n",
    "                torch.tensor(self.tokenizer.encode(neg_doc), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a26ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    queries, pos_docs, neg_docs = zip(*batch)\n",
    "    return (\n",
    "        pad_sequence(queries, batch_first=True),\n",
    "        pad_sequence(pos_docs, batch_first=True),\n",
    "        pad_sequence(neg_docs, batch_first=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800f97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dual RNN Encoder Model ---\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Load pretrained embeddings if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "            # Keep embeddings trainable (they are by default)\n",
    "            \n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.rnn(x)\n",
    "        return h_n.squeeze(0)  # shape: (batch, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23204e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Triplet Loss Function ---\n",
    "def triplet_loss_function(triplet, distance_function, margin):\n",
    "    query, pos_doc, neg_doc = triplet\n",
    "    d_pos = distance_function(query, pos_doc)\n",
    "    d_neg = distance_function(query, neg_doc)\n",
    "    return torch.clamp(d_pos - d_neg + margin, min=0.0).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Setup ---\n",
    "VOCAB_SIZE = tokenizer.vocab_size()\n",
    "\n",
    "# Load pretrained embeddings\n",
    "pretrained_embeddings = np.load(config['EMBEDDINGS_PATH'])\n",
    "EMBED_DIM = pretrained_embeddings.shape[1]  # Get embedding dimension from loaded embeddings\n",
    "\n",
    "print(f\"Loaded pretrained embeddings: {pretrained_embeddings.shape}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Embedding dimension: {EMBED_DIM}\")\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize encoders with pretrained embeddings and move to GPU\n",
    "query_encoder = RNNEncoder(VOCAB_SIZE, EMBED_DIM, config['HIDDEN_DIM'], pretrained_embeddings).to(device)\n",
    "doc_encoder = RNNEncoder(VOCAB_SIZE, EMBED_DIM, config['HIDDEN_DIM'], pretrained_embeddings).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(query_encoder.parameters()) + list(doc_encoder.parameters()), lr=config['LR'])\n",
    "\n",
    "# CRITICAL FIX: Increase batch size dramatically for much faster training\n",
    "dataset = TripletDataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=config['BATCH_SIZE'], shuffle=True, collate_fn=collate_fn, \n",
    "                       num_workers=2, pin_memory=True if device.type == 'mps' else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea93713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "import time\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config['EPOCHS']):\n",
    "    epoch_start = time.time()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for query_batch, pos_batch, neg_batch in dataloader:\n",
    "        # Move tensors to GPU\n",
    "        query_batch = query_batch.to(device)\n",
    "        pos_batch = pos_batch.to(device)\n",
    "        neg_batch = neg_batch.to(device)\n",
    "        \n",
    "        q_vec = query_encoder(query_batch)\n",
    "        pos_vec = doc_encoder(pos_batch)\n",
    "        neg_vec = doc_encoder(neg_batch)\n",
    "\n",
    "        loss = triplet_loss_function((q_vec, pos_vec, neg_vec), F.pairwise_distance, config['MARGIN'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Progress indicator every 50 batches\n",
    "        if num_batches % 50 == 0:\n",
    "            print(f\"  Batch {num_batches}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1}/{config['EPOCHS']}, Avg Loss: {avg_loss:.4f}, Time: {epoch_time:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed! Total time: {total_time/60:.1f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Automatic Model Saving ---\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create artifacts directory\n",
    "artifacts_dir = \"../artifacts\"\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Create timestamped run directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join(artifacts_dir, f\"two_tower_run_{timestamp}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üíæ Saving artifacts to: {run_dir}\")\n",
    "\n",
    "# Save model state dictionaries\n",
    "torch.save({\n",
    "    'query_encoder_state_dict': query_encoder.state_dict(),\n",
    "    'doc_encoder_state_dict': doc_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': config['EPOCHS'],\n",
    "    'final_loss': avg_loss\n",
    "}, os.path.join(run_dir, 'model_checkpoint.pth'))\n",
    "\n",
    "# Save model architectures (for easy loading later)\n",
    "torch.save(query_encoder, os.path.join(run_dir, 'query_encoder_full.pth'))\n",
    "torch.save(doc_encoder, os.path.join(run_dir, 'doc_encoder_full.pth'))\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    'model_config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embed_dim': EMBED_DIM,\n",
    "        'hidden_dim': config['HIDDEN_DIM'],\n",
    "        'margin': config['MARGIN']\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': config['EPOCHS'],\n",
    "        'batch_size': config['BATCH_SIZE'],\n",
    "        'learning_rate': config['LR'],\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'data_config': {\n",
    "        'train_samples': len(train_data),\n",
    "        'val_samples': len(val_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'total_triplets': len(train_data) + len(val_data) + len(test_data)\n",
    "    },\n",
    "    'training_results': {\n",
    "        'final_avg_loss': avg_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, 'training_config.json'), 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "# Save tokenizer (copy the word2idx file or save the object)\n",
    "import shutil\n",
    "if os.path.exists(config['WORD_TO_IDX_PATH']):\n",
    "    shutil.copy2(config['WORD_TO_IDX_PATH'], os.path.join(run_dir, 'word_to_idx.pkl'))\n",
    "\n",
    "print(f\"‚úÖ Saved artifacts:\")\n",
    "print(f\"  üìÅ Directory: {run_dir}\")\n",
    "print(f\"  üß† Models: model_checkpoint.pth, *_encoder_full.pth\")\n",
    "print(f\"  ‚öôÔ∏è  Config: training_config.json\")\n",
    "print(f\"  üìù Tokenizer: word_to_idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1362865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Function ---\n",
    "def search(query_text, documents, tokenizer, query_encoder, doc_encoder):\n",
    "    with torch.no_grad():\n",
    "        query_tensor = pad_sequence([torch.tensor(tokenizer.encode(query_text), dtype=torch.long)], batch_first=True)\n",
    "        query_vec = query_encoder(query_tensor)\n",
    "\n",
    "        doc_tensors = pad_sequence([torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents], batch_first=True)\n",
    "        doc_vecs = doc_encoder(doc_tensors)\n",
    "\n",
    "        scores = F.cosine_similarity(query_vec, doc_vecs)\n",
    "        top_indices = torch.argsort(scores, descending=True)\n",
    "        return [(documents[i], scores[i].item()) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448019c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comprehensive Testing with Real Data ---\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_retrieval(test_data, query_encoder, doc_encoder, tokenizer, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance using real test data\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE RETRIEVAL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Group test data by query to get all relevant docs per query\n",
    "    query_to_docs = defaultdict(list)\n",
    "    for query, pos_doc, neg_doc in test_data[:100]:  # Sample 100 for speed\n",
    "        query_to_docs[query].extend([pos_doc, neg_doc])\n",
    "    \n",
    "    # Test multiple queries\n",
    "    sample_queries = list(query_to_docs.keys())[:5]  # Test 5 queries\n",
    "    \n",
    "    for i, query in enumerate(sample_queries):\n",
    "        print(f\"\\nüîé TEST QUERY {i+1}: {query[:100]}...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get all documents for this query\n",
    "        documents = query_to_docs[query]\n",
    "        \n",
    "        # Add some random documents from other queries for harder test\n",
    "        other_docs = []\n",
    "        for other_query in random.sample(list(query_to_docs.keys()), 3):\n",
    "            if other_query != query:\n",
    "                other_docs.extend(query_to_docs[other_query][:2])\n",
    "        \n",
    "        all_documents = documents + other_docs\n",
    "        random.shuffle(all_documents)\n",
    "        \n",
    "        print(f\"üìö Searching through {len(all_documents)} documents...\")\n",
    "        \n",
    "        # Run search\n",
    "        results = search(query, all_documents, tokenizer, query_encoder, doc_encoder)\n",
    "        \n",
    "        print(f\"\\nüèÜ TOP {min(3, len(results))} RESULTS:\")\n",
    "        for j, (doc, score) in enumerate(results[:3]):\n",
    "            relevance = \"‚úÖ RELEVANT\" if doc in documents else \"‚ùå NOT RELEVANT\"\n",
    "            print(f\"{j+1}. Score: {score:.4f} {relevance}\")\n",
    "            print(f\"   Doc: {doc[:80]}...\")\n",
    "            print()\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluate_retrieval(test_data, query_encoder, doc_encoder, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
