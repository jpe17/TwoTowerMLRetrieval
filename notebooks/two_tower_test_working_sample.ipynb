{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6178d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import fastparquet\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import json\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "with open('../backend/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "def load_and_process_parquet(path):\n",
    "    \"\"\"Load a parquet file and create triplets with correct positive/negative logic.\"\"\"\n",
    "    print(f\"\\nProcessing {path}...\")\n",
    "    df = pd.read_parquet(path, engine='fastparquet')\n",
    "\n",
    "    # Filter for valid rows: keep queries that have at least one passage\n",
    "    valid_mask = (\n",
    "        df['query'].notna() & \n",
    "        df['passages.passage_text'].notna() &\n",
    "        df['passages.passage_text'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)\n",
    "    )\n",
    "    df = df[valid_mask].reset_index(drop=True)\n",
    "    print(f\"  Found {len(df)} valid queries.\")\n",
    "\n",
    "    # Create a flat list of (query_id, passage) for negative sampling\n",
    "    all_passages = []\n",
    "    for idx, row in df.iterrows():\n",
    "        for p in row['passages.passage_text']:\n",
    "            all_passages.append((idx, p))  # tag with query index for filtering later\n",
    "\n",
    "    triplets = []\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row['query']\n",
    "        query_passages = row['passages.passage_text']\n",
    "\n",
    "        for _ in range(config['NUM_TRIPLETS_PER_QUERY']): \n",
    "            positive = random.choice(query_passages)\n",
    "\n",
    "            # Negative must be from a *different* query\n",
    "            while True:\n",
    "                neg_query_id, negative = random.choice(all_passages)\n",
    "                if neg_query_id != idx:\n",
    "                    break\n",
    "\n",
    "            triplets.append({'query': query, 'positive': positive, 'negative': negative})\n",
    "\n",
    "    print(f\"  Generated {len(triplets)} triplets.\")\n",
    "    # Convert to list of tuples for TripletDataset\n",
    "    return [(t['query'], t['positive'], t['negative']) for t in triplets]\n",
    "\n",
    "# --- Tokenizer and Vocab ---\n",
    "class PretrainedTokenizer:\n",
    "    def __init__(self, word_to_idx_path):\n",
    "        # Load pretrained word_to_idx mapping\n",
    "        with open(word_to_idx_path, 'rb') as f:\n",
    "            self.word2idx = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded vocabulary with {len(self.word2idx):,} tokens\")\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        # Tokenize sentence, preserving punctuation.\n",
    "        tokens = re.findall(r\"\\w+|[.,!?;]\", str(sentence))\n",
    "        # Only include words that exist in vocabulary, skip unknown words\n",
    "        return [self.word2idx[word] for word in tokens if word in self.word2idx]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query, pos_doc, neg_doc = self.data[idx]\n",
    "        return (torch.tensor(self.tokenizer.encode(query), dtype=torch.long),\n",
    "                torch.tensor(self.tokenizer.encode(pos_doc), dtype=torch.long),\n",
    "                torch.tensor(self.tokenizer.encode(neg_doc), dtype=torch.long))\n",
    "\n",
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    queries, pos_docs, neg_docs = zip(*batch)\n",
    "    return (\n",
    "        pad_sequence(queries, batch_first=True, padding_value=0),\n",
    "        pad_sequence(pos_docs, batch_first=True, padding_value=0),\n",
    "        pad_sequence(neg_docs, batch_first=True, padding_value=0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35ca90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary with 400,000 tokens\n",
      "\n",
      "Creating DataLoaders...\n",
      "\n",
      "Processing ../data/ms_marco_train.parquet...\n",
      "  Found 808731 valid queries.\n",
      "  Generated 808731 triplets.\n",
      "✅ train dataloader ready!\n",
      "\n",
      "Processing ../data/ms_marco_validation.parquet...\n",
      "  Found 101093 valid queries.\n",
      "  Generated 101093 triplets.\n",
      "✅ validation dataloader ready!\n",
      "\n",
      "Processing ../data/ms_marco_test.parquet...\n",
      "  Found 101092 valid queries.\n",
      "  Generated 101092 triplets.\n",
      "✅ test dataloader ready!\n",
      "\n",
      "Dataset sizes:\n",
      "  Training: 808,731 triplets\n",
      "  Validation: 101,093 triplets\n",
      "  Test: 101,092 triplets\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer\n",
    "tokenizer = PretrainedTokenizer('../data/word_to_idx.pkl')\n",
    "\n",
    "# Dataset paths\n",
    "dataset_paths = {\n",
    "    'train': '../data/ms_marco_train.parquet',\n",
    "    'validation': '../data/ms_marco_validation.parquet',\n",
    "    'test': '../data/ms_marco_test.parquet'\n",
    "}\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataloaders = {}\n",
    "datasets = {}\n",
    "\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "for split, path in dataset_paths.items():\n",
    "    data_tuples = load_and_process_parquet(path)\n",
    "    datasets[split] = TripletDataset(data_tuples, tokenizer)\n",
    "    \n",
    "    # Use a smaller batch size for validation and test if needed\n",
    "    batch_size = 2 # You can adjust this\n",
    "\n",
    "    dataloaders[split] = DataLoader(\n",
    "        datasets[split],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(split == 'train'),\n",
    "        num_workers=0,  # Set to 0 for Jupyter notebooks to avoid multiprocessing issues\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=False  # Set to False for MPS compatibility\n",
    "    )\n",
    "    print(f\"✅ {split} dataloader ready!\")\n",
    "\n",
    "# Extract data for later use (e.g., evaluation)\n",
    "train_data = datasets['train'].data\n",
    "val_data = datasets['validation'].data\n",
    "test_data = datasets['test'].data\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_data):,} triplets\")\n",
    "print(f\"  Validation: {len(val_data):,} triplets\")\n",
    "print(f\"  Test: {len(test_data):,} triplets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1496b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS is available\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3c19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset Info:\n",
      "  Total batches in train: 404366\n",
      "  Batch size: 2\n",
      "\n",
      "🔍 Testing first few batches...\n",
      "\n",
      "Batch 1:\n",
      "  Query batch shape: torch.Size([2, 5])\n",
      "  Positive batch shape: torch.Size([2, 55])\n",
      "  Negative batch shape: torch.Size([2, 53])\n",
      "\n",
      "Batch 2:\n",
      "  Query batch shape: torch.Size([2, 6])\n",
      "  Positive batch shape: torch.Size([2, 49])\n",
      "  Negative batch shape: torch.Size([2, 80])\n",
      "\n",
      "Batch 3:\n",
      "  Query batch shape: torch.Size([2, 12])\n",
      "  Positive batch shape: torch.Size([2, 29])\n",
      "  Negative batch shape: torch.Size([2, 50])\n",
      "\n",
      "Batch 4:\n",
      "  Query batch shape: torch.Size([2, 8])\n",
      "  Positive batch shape: torch.Size([2, 81])\n",
      "  Negative batch shape: torch.Size([2, 49])\n",
      "\n",
      "✅ DataLoader test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the training dataloader\n",
    "train_loader = dataloaders['train']\n",
    "\n",
    "print(f\"📊 Dataset Info:\")\n",
    "print(f\"  Total batches in train: {len(train_loader)}\")\n",
    "print(f\"  Batch size: {train_loader.batch_size}\")\n",
    "\n",
    "print(\"\\n🔍 Testing first few batches...\")\n",
    "for batch_idx, (query_batch, pos_batch, neg_batch) in enumerate(train_loader):\n",
    "    print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "    print(f\"  Query batch shape: {query_batch.shape}\")\n",
    "    print(f\"  Positive batch shape: {pos_batch.shape}\")\n",
    "    print(f\"  Negative batch shape: {neg_batch.shape}\")\n",
    "    \n",
    "    if batch_idx >= 3:  # Just show first 4 batches\n",
    "        break\n",
    "\n",
    "print(\"\\n✅ DataLoader test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d19900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Dual RNN Encoder Model ---\n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pretrained_embeddings=None):\n",
    "        super().__init__()\n",
    "        # Use padding_idx=0 since we pad with 0\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Load pretrained embeddings if provided\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "            # Keep embeddings trainable (they are by default)\n",
    "            \n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, h_n = self.rnn(x)\n",
    "        return h_n.squeeze(0)  # shape: (batch, hidden_dim)\n",
    "\n",
    "# --- Triplet Loss Function ---\n",
    "def triplet_loss_function(triplet, distance_function, margin):\n",
    "    query, pos_doc, neg_doc = triplet\n",
    "    d_pos = distance_function(query, pos_doc)\n",
    "    d_neg = distance_function(query, neg_doc)\n",
    "    return torch.clamp(d_pos - d_neg + margin, min=0.0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea44b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocabulary with 400,000 tokens\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer\n",
    "tokenizer = PretrainedTokenizer('../data/word_to_idx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ba67ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 (disables memory limit)\n",
      "🧹 Memory cleaned\n",
      "Loaded pretrained embeddings: (400000, 200)\n",
      "Vocabulary size: 400000\n",
      "Embedding dimension: 200\n",
      "✅ MPS (Apple Silicon GPU) is available and will be used\n",
      "Selected device: mps\n",
      "Memory before model loading: MPS: Memory tracking not available\n",
      "🔄 Loading models to device...\n",
      "🧹 Memory cleaned\n",
      "✅ Query encoder loaded. Memory: MPS: Memory tracking not available\n",
      "✅ Doc encoder loaded. Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "🚀 Ready to train! Final memory: MPS: Memory tracking not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# 🔧 SET MPS MEMORY ENVIRONMENT VARIABLE\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "print(\"🔧 Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 (disables memory limit)\")\n",
    "\n",
    "# 🧹 MEMORY MANAGEMENT UTILITIES\n",
    "def clean_memory():\n",
    "    \"\"\"Aggressive memory cleaning for MPS/CUDA\"\"\"\n",
    "    gc.collect()  # Python garbage collection\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "            torch.mps.synchronize()\n",
    "        except AttributeError:\n",
    "            # Older PyTorch versions might not have these methods\n",
    "            pass\n",
    "    print(\"🧹 Memory cleaned\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return f\"CUDA: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.memory_reserved()/1024**3:.2f}GB\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        # MPS doesn't have direct memory query methods, so we return a placeholder\n",
    "        return \"MPS: Memory tracking not available\"\n",
    "    return \"CPU: Memory tracking not available\"\n",
    "\n",
    "# Clean memory before starting\n",
    "clean_memory()\n",
    "\n",
    "with open('../backend/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# --- Training Setup ---\n",
    "VOCAB_SIZE = tokenizer.vocab_size()\n",
    "\n",
    "# Load pretrained embeddings\n",
    "pretrained_embeddings = np.load('../data/embeddings.npy')\n",
    "EMBED_DIM = pretrained_embeddings.shape[1]  # Get embedding dimension from loaded embeddings\n",
    "\n",
    "print(f\"Loaded pretrained embeddings: {pretrained_embeddings.shape}\")\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Embedding dimension: {EMBED_DIM}\")\n",
    "\n",
    "# Enhanced device selection with better fallback handling\n",
    "def get_best_device():\n",
    "    \"\"\"Get the best available device with informative feedback.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"✅ MPS (Apple Silicon GPU) is available and will be used\")\n",
    "        return torch.device('mps')\n",
    "    elif torch.cuda.is_available():\n",
    "        print(\"✅ CUDA GPU is available and will be used\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        print(\"⚠️  No GPU acceleration available - using CPU\")\n",
    "        print(\"   💡 For better performance on Apple Silicon, run outside Docker\")\n",
    "        print(\"   💡 For CUDA support, configure Docker with GPU passthrough\")\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_best_device()\n",
    "print(f\"Selected device: {device}\")\n",
    "print(f\"Memory before model loading: {get_memory_usage()}\")\n",
    "\n",
    "# Initialize encoders with pretrained embeddings and move to GPU\n",
    "print(\"🔄 Loading models to device...\")\n",
    "clean_memory()  # Clean before loading models\n",
    "\n",
    "query_encoder = RNNEncoder(VOCAB_SIZE, EMBED_DIM, config['HIDDEN_DIM'], pretrained_embeddings).to(device)\n",
    "print(f\"✅ Query encoder loaded. Memory: {get_memory_usage()}\")\n",
    "\n",
    "doc_encoder = RNNEncoder(VOCAB_SIZE, EMBED_DIM, config['HIDDEN_DIM'], pretrained_embeddings).to(device)\n",
    "print(f\"✅ Doc encoder loaded. Memory: {get_memory_usage()}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(list(query_encoder.parameters()) + list(doc_encoder.parameters()), lr=config['LR'])\n",
    "\n",
    "# The dataloader is already created, we just need to adjust batch size in the config\n",
    "# and re-run the dataloader creation cell if we want to change it.\n",
    "config['BATCH_SIZE'] = dataloaders['train'].batch_size\n",
    "train_loader = dataloaders['train']\n",
    "\n",
    "clean_memory()  # Final cleanup\n",
    "print(f\"🚀 Ready to train! Final memory: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting memory-optimized training...\n",
      "   📊 Gradient accumulation steps: 4\n",
      "   🧹 Memory cleanup every 100 batches\n",
      "  Batch 50/404366, Loss: 1.6608, Memory: MPS: Memory tracking not available\n",
      "  Batch 100/404366, Loss: 0.9258, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 150/404366, Loss: 1.0752, Memory: MPS: Memory tracking not available\n",
      "  Batch 200/404366, Loss: 0.9413, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 250/404366, Loss: 1.0299, Memory: MPS: Memory tracking not available\n",
      "  Batch 300/404366, Loss: 1.3037, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 350/404366, Loss: 1.5755, Memory: MPS: Memory tracking not available\n",
      "  Batch 400/404366, Loss: 0.9334, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 450/404366, Loss: 0.8289, Memory: MPS: Memory tracking not available\n",
      "  Batch 500/404366, Loss: 1.0617, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 550/404366, Loss: 0.6967, Memory: MPS: Memory tracking not available\n",
      "  Batch 600/404366, Loss: 0.7718, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 650/404366, Loss: 1.1065, Memory: MPS: Memory tracking not available\n",
      "  Batch 700/404366, Loss: 0.7156, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 750/404366, Loss: 0.0226, Memory: MPS: Memory tracking not available\n",
      "  Batch 800/404366, Loss: 1.1709, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 850/404366, Loss: 0.9426, Memory: MPS: Memory tracking not available\n",
      "  Batch 900/404366, Loss: 1.2841, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 950/404366, Loss: 0.5239, Memory: MPS: Memory tracking not available\n",
      "  Batch 1000/404366, Loss: 1.3601, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1050/404366, Loss: 1.4080, Memory: MPS: Memory tracking not available\n",
      "  Batch 1100/404366, Loss: 0.5828, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1150/404366, Loss: 0.9022, Memory: MPS: Memory tracking not available\n",
      "  Batch 1200/404366, Loss: 0.5718, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1250/404366, Loss: 0.8999, Memory: MPS: Memory tracking not available\n",
      "  Batch 1300/404366, Loss: 0.1548, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1350/404366, Loss: 0.6727, Memory: MPS: Memory tracking not available\n",
      "  Batch 1400/404366, Loss: 0.6127, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1450/404366, Loss: 0.7674, Memory: MPS: Memory tracking not available\n",
      "  Batch 1500/404366, Loss: 0.3097, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1550/404366, Loss: 1.0783, Memory: MPS: Memory tracking not available\n",
      "  Batch 1600/404366, Loss: 1.3501, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1650/404366, Loss: 0.2708, Memory: MPS: Memory tracking not available\n",
      "  Batch 1700/404366, Loss: 0.4425, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1750/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "  Batch 1800/404366, Loss: 1.2988, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1850/404366, Loss: 0.4760, Memory: MPS: Memory tracking not available\n",
      "  Batch 1900/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 1950/404366, Loss: 0.4748, Memory: MPS: Memory tracking not available\n",
      "  Batch 2000/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2050/404366, Loss: 0.0731, Memory: MPS: Memory tracking not available\n",
      "  Batch 2100/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2150/404366, Loss: 1.4733, Memory: MPS: Memory tracking not available\n",
      "  Batch 2200/404366, Loss: 0.8259, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2250/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "  Batch 2300/404366, Loss: 0.3650, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2350/404366, Loss: 0.8798, Memory: MPS: Memory tracking not available\n",
      "  Batch 2400/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2450/404366, Loss: 0.2023, Memory: MPS: Memory tracking not available\n",
      "  Batch 2500/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2550/404366, Loss: 1.2224, Memory: MPS: Memory tracking not available\n",
      "  Batch 2600/404366, Loss: 0.3493, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2650/404366, Loss: 0.9856, Memory: MPS: Memory tracking not available\n",
      "  Batch 2700/404366, Loss: 0.9793, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2750/404366, Loss: 0.1746, Memory: MPS: Memory tracking not available\n",
      "  Batch 2800/404366, Loss: 0.6776, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2850/404366, Loss: 0.0000, Memory: MPS: Memory tracking not available\n",
      "  Batch 2900/404366, Loss: 0.8813, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 2950/404366, Loss: 0.6185, Memory: MPS: Memory tracking not available\n",
      "  Batch 3000/404366, Loss: 0.3484, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 3050/404366, Loss: 0.7050, Memory: MPS: Memory tracking not available\n",
      "  Batch 3100/404366, Loss: 1.0070, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 3150/404366, Loss: 0.6770, Memory: MPS: Memory tracking not available\n",
      "  Batch 3200/404366, Loss: 0.6596, Memory: MPS: Memory tracking not available\n",
      "🧹 Memory cleaned\n",
      "  Batch 3250/404366, Loss: 0.3432, Memory: MPS: Memory tracking not available\n"
     ]
    }
   ],
   "source": [
    "# --- Memory-Optimized Training Loop with Gradient Accumulation ---\n",
    "import time\n",
    "\n",
    "# Training configuration for memory optimization\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Accumulate gradients over 4 steps\n",
    "MEMORY_CLEANUP_FREQUENCY = 100   # Clean memory every 100 batches\n",
    "\n",
    "print(\"🚀 Starting memory-optimized training...\")\n",
    "print(f\"   📊 Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   🧹 Memory cleanup every {MEMORY_CLEANUP_FREQUENCY} batches\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(config['EPOCHS']):\n",
    "    epoch_start = time.time()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    accumulation_loss = 0\n",
    "    \n",
    "    # Set models to training mode\n",
    "    query_encoder.train()\n",
    "    doc_encoder.train()\n",
    "    \n",
    "    for batch_idx, (query_batch, pos_batch, neg_batch) in enumerate(train_loader):\n",
    "        try:\n",
    "            # Move tensors to device\n",
    "            query_batch = query_batch.to(device, non_blocking=True)\n",
    "            pos_batch = pos_batch.to(device, non_blocking=True)\n",
    "            neg_batch = neg_batch.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            q_vec = query_encoder(query_batch)\n",
    "            pos_vec = doc_encoder(pos_batch)\n",
    "            neg_vec = doc_encoder(neg_batch)\n",
    "\n",
    "            loss = triplet_loss_function((q_vec, pos_vec, neg_vec), F.pairwise_distance, config['MARGIN'])\n",
    "            \n",
    "            # Scale loss for gradient accumulation\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            accumulation_loss += loss.item()\n",
    "            \n",
    "            # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "            if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += accumulation_loss * GRADIENT_ACCUMULATION_STEPS\n",
    "                accumulation_loss = 0\n",
    "            \n",
    "            num_batches += 1\n",
    "            \n",
    "            # Progress indicator and memory management\n",
    "            if num_batches % 50 == 0:\n",
    "                current_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "                print(f\"  Batch {num_batches}/{len(train_loader)}, Loss: {current_loss:.4f}, Memory: {get_memory_usage()}\")\n",
    "            \n",
    "            # Periodic memory cleanup\n",
    "            if num_batches % MEMORY_CLEANUP_FREQUENCY == 0:\n",
    "                clean_memory()\n",
    "                \n",
    "            # Clear intermediate tensors\n",
    "            del query_batch, pos_batch, neg_batch, q_vec, pos_vec, neg_vec, loss\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"❌ Memory error at batch {num_batches}: {str(e)}\")\n",
    "                print(\"🧹 Attempting memory cleanup and continuing...\")\n",
    "                clean_memory()\n",
    "                optimizer.zero_grad()  # Clear gradients\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Handle any remaining accumulated gradients\n",
    "    if accumulation_loss > 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += accumulation_loss * GRADIENT_ACCUMULATION_STEPS\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    print(f\"✅ Epoch {epoch+1}/{config['EPOCHS']}, Avg Loss: {avg_loss:.4f}, Time: {epoch_time:.1f}s\")\n",
    "    print(f\"   Memory after epoch: {get_memory_usage()}\")\n",
    "    \n",
    "    # Clean memory after each epoch\n",
    "    clean_memory()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n🎉 Training completed! Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Final memory state: {get_memory_usage()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 TEST: Verify Environment and Memory Management\n",
    "print(\"🧪 Testing memory management setup...\")\n",
    "print(f\"✅ PYTORCH_MPS_HIGH_WATERMARK_RATIO = {os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO', 'NOT SET')}\")\n",
    "print(f\"✅ Current device: {device}\")\n",
    "print(f\"✅ Current memory state: {get_memory_usage()}\")\n",
    "\n",
    "# Test memory cleaning\n",
    "print(\"\\n🧹 Testing memory cleaning...\")\n",
    "clean_memory()\n",
    "print(\"✅ Memory cleaning completed\")\n",
    "\n",
    "# Test a small forward pass to verify everything works\n",
    "print(\"\\n🔬 Testing small forward pass...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Create small test tensors\n",
    "        test_query = torch.randint(1, 1000, (1, 10)).to(device)\n",
    "        test_doc = torch.randint(1, 1000, (1, 15)).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        q_vec = query_encoder(test_query)\n",
    "        d_vec = doc_encoder(test_doc)\n",
    "        \n",
    "        print(f\"✅ Forward pass successful!\")\n",
    "        print(f\"   Query vector shape: {q_vec.shape}\")\n",
    "        print(f\"   Doc vector shape: {d_vec.shape}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del test_query, test_doc, q_vec, d_vec\n",
    "        clean_memory()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Forward pass failed: {str(e)}\")\n",
    "\n",
    "print(\"\\n🎯 Setup verification complete! You can now run the training cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23204e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Automatic Model Saving ---\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create artifacts directory\n",
    "artifacts_dir = \"../artifacts\"\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Create timestamped run directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join(artifacts_dir, f\"two_tower_run_{timestamp}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"💾 Saving artifacts to: {run_dir}\")\n",
    "\n",
    "# Save model state dictionaries\n",
    "torch.save({\n",
    "    'query_encoder_state_dict': query_encoder.state_dict(),\n",
    "    'doc_encoder_state_dict': doc_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': config['EPOCHS'],\n",
    "    'final_loss': avg_loss\n",
    "}, os.path.join(run_dir, 'model_checkpoint.pth'))\n",
    "\n",
    "# Save model architectures (for easy loading later)\n",
    "torch.save(query_encoder, os.path.join(run_dir, 'query_encoder_full.pth'))\n",
    "torch.save(doc_encoder, os.path.join(run_dir, 'doc_encoder_full.pth'))\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    'model_config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embed_dim': EMBED_DIM,\n",
    "        'hidden_dim': config['HIDDEN_DIM'],\n",
    "        'margin': config['MARGIN']\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': config['EPOCHS'],\n",
    "        'batch_size': config['BATCH_SIZE'],\n",
    "        'learning_rate': config['LR'],\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'data_config': {\n",
    "        'train_samples': len(train_data),\n",
    "        'val_samples': len(val_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'total_triplets': len(train_data) + len(val_data) + len(test_data)\n",
    "    },\n",
    "    'training_results': {\n",
    "        'final_avg_loss': avg_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, 'training_config.json'), 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "# Save tokenizer (copy the word2idx file or save the object)\n",
    "import shutil\n",
    "if os.path.exists(config['WORD_TO_IDX_PATH']):\n",
    "    shutil.copy2(config['WORD_TO_IDX_PATH'], os.path.join(run_dir, 'word_to_idx.pkl'))\n",
    "\n",
    "print(f\"✅ Saved artifacts:\")\n",
    "print(f\"  📁 Directory: {run_dir}\")\n",
    "print(f\"  🧠 Models: model_checkpoint.pth, *_encoder_full.pth\")\n",
    "print(f\"  ⚙️  Config: training_config.json\")\n",
    "print(f\"  📝 Tokenizer: word_to_idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b53d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained embeddings: (400000, 200)\n",
      "Vocabulary size: 400000\n",
      "Embedding dimension: 200\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- MPS-Optimized Inference Function ---\n",
    "def search(query_text, documents, tokenizer, query_encoder, doc_encoder):\n",
    "    \"\"\"\n",
    "    Search function optimized for MPS (Apple Silicon GPU)\n",
    "    \"\"\"\n",
    "    # Get device from encoder\n",
    "    device = next(query_encoder.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode query and move to device\n",
    "        query_tensor = pad_sequence([torch.tensor(tokenizer.encode(query_text), dtype=torch.long)], batch_first=True).to(device)\n",
    "        query_vec = query_encoder(query_tensor)\n",
    "\n",
    "        # Encode documents and move to device\n",
    "        doc_tensors = pad_sequence([torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents], batch_first=True).to(device)\n",
    "        doc_vecs = doc_encoder(doc_tensors)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        scores = F.cosine_similarity(query_vec, doc_vecs)\n",
    "        top_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        # Convert results back to CPU for return (if needed)\n",
    "        results = [(documents[i], scores[i].item()) for i in top_indices]\n",
    "        \n",
    "        # Clear MPS cache after inference if available\n",
    "        if device.type == 'mps' and hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea93713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training...\n",
      "  Batch 50/126365, Loss: 1.0012\n",
      "  Batch 100/126365, Loss: 0.9825\n"
     ]
    }
   ],
   "source": [
    "# --- Comprehensive Testing with Real Data ---\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_retrieval(test_data, query_encoder, doc_encoder, tokenizer, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance using real test data\n",
    "    \"\"\"\n",
    "    print(\"🔍 COMPREHENSIVE RETRIEVAL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Group test data by query to get all relevant docs per query\n",
    "    query_to_docs = defaultdict(list)\n",
    "    for query, pos_doc, neg_doc in test_data[:100]:  # Sample 100 for speed\n",
    "        query_to_docs[query].extend([pos_doc, neg_doc])\n",
    "    \n",
    "    # Test multiple queries\n",
    "    sample_queries = list(query_to_docs.keys())[:5]  # Test 5 queries\n",
    "    \n",
    "    for i, query in enumerate(sample_queries):\n",
    "        print(f\"\\n🔎 TEST QUERY {i+1}: {query[:100]}...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get all documents for this query\n",
    "        documents = query_to_docs[query]\n",
    "        \n",
    "        # Add some random documents from other queries for harder test\n",
    "        other_docs = []\n",
    "        for other_query in random.sample(list(query_to_docs.keys()), 3):\n",
    "            if other_query != query:\n",
    "                other_docs.extend(query_to_docs[other_query][:2])\n",
    "        \n",
    "        all_documents = documents + other_docs\n",
    "        random.shuffle(all_documents)\n",
    "        \n",
    "        print(f\"📚 Searching through {len(all_documents)} documents...\")\n",
    "        \n",
    "        # Run search\n",
    "        results = search(query, all_documents, tokenizer, query_encoder, doc_encoder)\n",
    "        \n",
    "        print(f\"\\n🏆 TOP {min(3, len(results))} RESULTS:\")\n",
    "        for j, (doc, score) in enumerate(results[:3]):\n",
    "            relevance = \"✅ RELEVANT\" if doc in documents else \"❌ NOT RELEVANT\"\n",
    "            print(f\"{j+1}. Score: {score:.4f} {relevance}\")\n",
    "            print(f\"   Doc: {doc[:80]}...\")\n",
    "            print()\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluate_retrieval(test_data, query_encoder, doc_encoder, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Automatic Model Saving ---\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create artifacts directory\n",
    "artifacts_dir = \"../artifacts\"\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Create timestamped run directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join(artifacts_dir, f\"two_tower_run_{timestamp}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"💾 Saving artifacts to: {run_dir}\")\n",
    "\n",
    "# Save model state dictionaries\n",
    "torch.save({\n",
    "    'query_encoder_state_dict': query_encoder.state_dict(),\n",
    "    'doc_encoder_state_dict': doc_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': config['EPOCHS'],\n",
    "    'final_loss': avg_loss\n",
    "}, os.path.join(run_dir, 'model_checkpoint.pth'))\n",
    "\n",
    "# Save model architectures (for easy loading later)\n",
    "torch.save(query_encoder, os.path.join(run_dir, 'query_encoder_full.pth'))\n",
    "torch.save(doc_encoder, os.path.join(run_dir, 'doc_encoder_full.pth'))\n",
    "\n",
    "# Save training configuration\n",
    "training_config = {\n",
    "    'model_config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embed_dim': EMBED_DIM,\n",
    "        'hidden_dim': config['HIDDEN_DIM'],\n",
    "        'margin': config['MARGIN']\n",
    "    },\n",
    "    'training_config': {\n",
    "        'epochs': config['EPOCHS'],\n",
    "        'batch_size': config['BATCH_SIZE'],\n",
    "        'learning_rate': config['LR'],\n",
    "        'device': str(device)\n",
    "    },\n",
    "    'data_config': {\n",
    "        'train_samples': len(train_data),\n",
    "        'val_samples': len(val_data),\n",
    "        'test_samples': len(test_data),\n",
    "        'total_triplets': len(train_data) + len(val_data) + len(test_data)\n",
    "    },\n",
    "    'training_results': {\n",
    "        'final_avg_loss': avg_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, 'training_config.json'), 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "# Save tokenizer (copy the word2idx file or save the object)\n",
    "import shutil\n",
    "if os.path.exists(config['WORD_TO_IDX_PATH']):\n",
    "    shutil.copy2(config['WORD_TO_IDX_PATH'], os.path.join(run_dir, 'word_to_idx.pkl'))\n",
    "\n",
    "print(f\"✅ Saved artifacts:\")\n",
    "print(f\"  📁 Directory: {run_dir}\")\n",
    "print(f\"  🧠 Models: model_checkpoint.pth, *_encoder_full.pth\")\n",
    "print(f\"  ⚙️  Config: training_config.json\")\n",
    "print(f\"  📝 Tokenizer: word_to_idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1362865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MPS-Optimized Inference Function ---\n",
    "def search(query_text, documents, tokenizer, query_encoder, doc_encoder):\n",
    "    \"\"\"\n",
    "    Search function optimized for MPS (Apple Silicon GPU)\n",
    "    \"\"\"\n",
    "    # Get device from encoder\n",
    "    device = next(query_encoder.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode query and move to device\n",
    "        query_tensor = pad_sequence([torch.tensor(tokenizer.encode(query_text), dtype=torch.long)], batch_first=True).to(device)\n",
    "        query_vec = query_encoder(query_tensor)\n",
    "\n",
    "        # Encode documents and move to device\n",
    "        doc_tensors = pad_sequence([torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents], batch_first=True).to(device)\n",
    "        doc_vecs = doc_encoder(doc_tensors)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        scores = F.cosine_similarity(query_vec, doc_vecs)\n",
    "        top_indices = torch.argsort(scores, descending=True)\n",
    "        \n",
    "        # Convert results back to CPU for return (if needed)\n",
    "        results = [(documents[i], scores[i].item()) for i in top_indices]\n",
    "        \n",
    "        # Clear MPS cache after inference if available\n",
    "        if device.type == 'mps' and hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448019c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comprehensive Testing with Real Data ---\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_retrieval(test_data, query_encoder, doc_encoder, tokenizer, k=10):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance using real test data\n",
    "    \"\"\"\n",
    "    print(\"🔍 COMPREHENSIVE RETRIEVAL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Group test data by query to get all relevant docs per query\n",
    "    query_to_docs = defaultdict(list)\n",
    "    for query, pos_doc, neg_doc in test_data[:100]:  # Sample 100 for speed\n",
    "        query_to_docs[query].extend([pos_doc, neg_doc])\n",
    "    \n",
    "    # Test multiple queries\n",
    "    sample_queries = list(query_to_docs.keys())[:5]  # Test 5 queries\n",
    "    \n",
    "    for i, query in enumerate(sample_queries):\n",
    "        print(f\"\\n🔎 TEST QUERY {i+1}: {query[:100]}...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get all documents for this query\n",
    "        documents = query_to_docs[query]\n",
    "        \n",
    "        # Add some random documents from other queries for harder test\n",
    "        other_docs = []\n",
    "        for other_query in random.sample(list(query_to_docs.keys()), 3):\n",
    "            if other_query != query:\n",
    "                other_docs.extend(query_to_docs[other_query][:2])\n",
    "        \n",
    "        all_documents = documents + other_docs\n",
    "        random.shuffle(all_documents)\n",
    "        \n",
    "        print(f\"📚 Searching through {len(all_documents)} documents...\")\n",
    "        \n",
    "        # Run search\n",
    "        results = search(query, all_documents, tokenizer, query_encoder, doc_encoder)\n",
    "        \n",
    "        print(f\"\\n🏆 TOP {min(3, len(results))} RESULTS:\")\n",
    "        for j, (doc, score) in enumerate(results[:3]):\n",
    "            relevance = \"✅ RELEVANT\" if doc in documents else \"❌ NOT RELEVANT\"\n",
    "            print(f\"{j+1}. Score: {score:.4f} {relevance}\")\n",
    "            print(f\"   Doc: {doc[:80]}...\")\n",
    "            print()\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluate_retrieval(test_data, query_encoder, doc_encoder, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac337bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
