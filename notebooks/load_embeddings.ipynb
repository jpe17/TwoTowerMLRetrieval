{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataset to embeddings\n",
    "1. Convert MS Marco to format with query, 10 positive samples, and 10 negative samples \n",
    "2. Convert each of those to embeddings via Glove pre-trained model\n",
    "3. Prepare input torch tensors for training ML Two Towers model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Initialising functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 MS MARCO TO CONTRASTIVE TABLE CONVERSION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def extract_positive_passages_vectorized(row) -> List[str]:\n",
    "    \"\"\"Extract passages where is_selected = 1 - vectorized version\"\"\"\n",
    "    is_selected = row['passages.is_selected']\n",
    "    passage_texts = row['passages.passage_text']\n",
    "    \n",
    "    if isinstance(is_selected, list) and isinstance(passage_texts, list):\n",
    "        # Use list comprehension for speed\n",
    "        return [passage_texts[i] for i, selected in enumerate(is_selected) \n",
    "                if selected == 1 and i < len(passage_texts)]\n",
    "    return []\n",
    "\n",
    "def precompute_data_structures(df_filtered: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Pre-compute data structures for fast access\"\"\"\n",
    "    print(\"🔧 Pre-computing data structures for speed...\")\n",
    "    \n",
    "    # 1. Group by query for fast lookup\n",
    "    query_groups = df_filtered.groupby('query')\n",
    "    \n",
    "    # 2. Pre-compute all passages for each query\n",
    "    query_to_positives = {}\n",
    "    all_negative_passages = []\n",
    "    \n",
    "    for query, group in query_groups:\n",
    "        # Get all positive passages for this query\n",
    "        positives = []\n",
    "        for _, row in group.iterrows():\n",
    "            positives.extend(extract_positive_passages_vectorized(row))\n",
    "        query_to_positives[query] = positives\n",
    "        \n",
    "        # Collect all passages as potential negatives\n",
    "        for _, row in group.iterrows():\n",
    "            passages = row['passages.passage_text']\n",
    "            if isinstance(passages, list):\n",
    "                all_negative_passages.extend(passages)\n",
    "    \n",
    "    # 3. Create lookup for query metadata\n",
    "    query_metadata = df_filtered.groupby('query')[['query_type']].first().to_dict()['query_type']\n",
    "    \n",
    "    print(f\"✅ Pre-computed data for {len(query_to_positives)} queries\")\n",
    "    print(f\"✅ Collected {len(all_negative_passages)} total passages for negatives\")\n",
    "    \n",
    "    return query_to_positives, all_negative_passages, query_metadata\n",
    "\n",
    "def create_contrastive_table_optimized(df_filtered: pd.DataFrame, num_pos: int = 10, num_neg: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    OPTIMIZED: Convert MS MARCO dataframe to contrastive table\n",
    "    \"\"\"\n",
    "    print(\"🚀 Converting MS MARCO to contrastive table (OPTIMIZED)...\")\n",
    "    \n",
    "    # Pre-compute everything once\n",
    "    query_to_positives, all_negative_passages, query_metadata = precompute_data_structures(df_filtered)\n",
    "    \n",
    "    # Get unique queries\n",
    "    unique_queries = list(query_to_positives.keys())\n",
    "    print(f\"Processing {len(unique_queries)} unique queries\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process queries with vectorized operations\n",
    "    for idx, query in enumerate(unique_queries):\n",
    "        query_type = query_metadata[query]\n",
    "        all_positives = query_to_positives[query]\n",
    "        \n",
    "        # Sample positive passages\n",
    "        if len(all_positives) >= num_pos:\n",
    "            sampled_positives = random.sample(all_positives, num_pos)\n",
    "        elif all_positives:\n",
    "            # Repeat if not enough\n",
    "            sampled_positives = (all_positives * (num_pos // len(all_positives) + 1))[:num_pos]\n",
    "        else:\n",
    "            # Fallback: use query itself\n",
    "            sampled_positives = [query] * num_pos\n",
    "        \n",
    "        # Sample negative passages from pre-computed pool\n",
    "        if len(all_negative_passages) >= num_neg:\n",
    "            sampled_negatives = random.sample(all_negative_passages, num_neg)\n",
    "        else:\n",
    "            # Fallback if not enough negatives\n",
    "            sampled_negatives = (all_negative_passages * (num_neg // len(all_negative_passages) + 1))[:num_neg]\n",
    "        \n",
    "        # Create target arrays for different encoder types\n",
    "        dual_encoder_pos_targets = [1] * num_pos  # [1,1,1,1,1,1,1,1,1,1]\n",
    "        dual_encoder_neg_targets = [0] * num_neg  # [0,0,0,0,0,0,0,0,0,0]\n",
    "        \n",
    "        combined_encoder_pos_targets = [1] * num_pos   # [1,1,1,1,1,1,1,1,1,1] \n",
    "        combined_encoder_neg_targets = [0] * num_neg  # [-1,-1,-1,-1,-1,-1,-1,-1,-1,-1]\n",
    "        \n",
    "        # Create row\n",
    "        results.append({\n",
    "            'Query': query,\n",
    "            'Query_Type': query_type,\n",
    "            'Pos': sampled_positives,\n",
    "            'Neg': sampled_negatives,\n",
    "            'Dual_Encoder_Pos_Targets': dual_encoder_pos_targets,\n",
    "            'Dual_Encoder_Neg_Targets': dual_encoder_neg_targets,\n",
    "            'Combined_Encoder_Pos_Targets': combined_encoder_pos_targets,\n",
    "            'Combined_Encoder_Neg_Targets': combined_encoder_neg_targets\n",
    "        })\n",
    "        \n",
    "        if idx % 500 == 0:  # Less frequent updates for speed\n",
    "            print(f\"  Processed {idx}/{len(unique_queries)} queries\")\n",
    "    \n",
    "    contrastive_df = pd.DataFrame(results)\n",
    "    print(f\"✅ Created contrastive table with {len(contrastive_df)} rows\")\n",
    "    \n",
    "    return contrastive_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: Get MS Marco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "📊 Sampling 10,000 samples...\n",
      "Sampled: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Loading Raw\n",
    "NUMBER_OF_SAMPLES = 10000\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# Load and sample data FIRST\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_parquet(\"../data/ms_marco_train.parquet\", engine='fastparquet')\n",
    "\n",
    "print(f\"📊 Sampling {NUMBER_OF_SAMPLES:,} samples...\")\n",
    "\n",
    "df_sample = df.sample(n=NUMBER_OF_SAMPLES, random_state=42).copy()\n",
    "print(f\"Sampled: {len(df_sample)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Transform MS Marco Dataset and include negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n",
      "After filtering: 6146 samples\n",
      "🚀 Converting MS MARCO to contrastive table (OPTIMIZED)...\n",
      "🔧 Pre-computing data structures for speed...\n",
      "✅ Pre-computed data for 6146 queries\n",
      "✅ Collected 61265 total passages for negatives\n",
      "Processing 6146 unique queries\n",
      "  Processed 0/6146 queries\n",
      "  Processed 500/6146 queries\n",
      "  Processed 1000/6146 queries\n",
      "  Processed 1500/6146 queries\n",
      "  Processed 2000/6146 queries\n",
      "  Processed 2500/6146 queries\n",
      "  Processed 3000/6146 queries\n",
      "  Processed 3500/6146 queries\n",
      "  Processed 4000/6146 queries\n",
      "  Processed 4500/6146 queries\n",
      "  Processed 5000/6146 queries\n",
      "  Processed 5500/6146 queries\n",
      "  Processed 6000/6146 queries\n",
      "✅ Created contrastive table with 122920 rows\n",
      "\\n🎯 Sample of the contrastive table:\n",
      "Columns: ['Query', 'Query_Type', 'Passage', 'Target']\n",
      "Shape: (122920, 4)\n",
      "\\n✅ Contrastive table ready!\n",
      "                   Query   Query_Type  \\\n",
      "0  'for hire' definition  DESCRIPTION   \n",
      "1  'for hire' definition  DESCRIPTION   \n",
      "2  'for hire' definition  DESCRIPTION   \n",
      "3  'for hire' definition  DESCRIPTION   \n",
      "4  'for hire' definition  DESCRIPTION   \n",
      "5  'for hire' definition  DESCRIPTION   \n",
      "6  'for hire' definition  DESCRIPTION   \n",
      "7  'for hire' definition  DESCRIPTION   \n",
      "8  'for hire' definition  DESCRIPTION   \n",
      "9  'for hire' definition  DESCRIPTION   \n",
      "\n",
      "                                             Passage  Target  \n",
      "0  1 hire-As a noun, it originally meant the paym...       1  \n",
      "1  1 hire-As a noun, it originally meant the paym...       1  \n",
      "2  1 hire-As a noun, it originally meant the paym...       1  \n",
      "3  1 hire-As a noun, it originally meant the paym...       1  \n",
      "4  1 hire-As a noun, it originally meant the paym...       1  \n",
      "5  1 hire-As a noun, it originally meant the paym...       1  \n",
      "6  1 hire-As a noun, it originally meant the paym...       1  \n",
      "7  1 hire-As a noun, it originally meant the paym...       1  \n",
      "8  1 hire-As a noun, it originally meant the paym...       1  \n",
      "9  1 hire-As a noun, it originally meant the paym...       1  \n"
     ]
    }
   ],
   "source": [
    "# Apply filtering AFTER sampling\n",
    "print(\"Filtering data...\")\n",
    "df_filtered = df_sample[\n",
    "         (df_sample['query'].notna()) &\n",
    "         (df_sample['query_id'].notna()) &\n",
    "         (df_sample['query_type'].notna()) &\n",
    "         (df_sample['passages.is_selected'].notna()) &\n",
    "         (df_sample['passages.is_selected'].apply(lambda x: 1 in x))\n",
    "     ].copy()\n",
    "\n",
    "print(f\"After filtering: {len(df_filtered)} samples\")\n",
    "\n",
    "# Create the simple contrastive table using OPTIMIZED version\n",
    "contrastive_table = create_contrastive_table_optimized(df_filtered, num_pos=10, num_neg=10)\n",
    "\n",
    "print(\"\\\\n🎯 Sample of the contrastive table:\")\n",
    "print(f\"Columns: {list(contrastive_table.columns)}\")\n",
    "print(f\"Shape: {contrastive_table.shape}\")\n",
    "\n",
    "\n",
    "print(\"\\\\n✅ Contrastive table ready!\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(contrastive_table.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4**: Embed all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    \"\"\"Load cached GloVe embeddings.\"\"\"\n",
    "    cache_dir = os.path.dirname(\"../data/\")\n",
    "    word_to_idx_path = os.path.join(cache_dir, \"word_to_idx.pkl\")\n",
    "    embeddings_path = os.path.join(cache_dir, \"embeddings.npy\")\n",
    "    \n",
    "    if os.path.exists(word_to_idx_path) and os.path.exists(embeddings_path):\n",
    "        with open(word_to_idx_path, 'rb') as f:\n",
    "            word_to_idx = pickle.load(f)\n",
    "        embeddings = np.load(embeddings_path)\n",
    "        return word_to_idx, embeddings\n",
    "    \n",
    "    print(\"❌ GloVe cache not found. Please run original data processing once to create cache.\")\n",
    "    return None, None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for embedding lookup.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "def query_to_embedding(query, word_to_idx, embeddings):\n",
    "    \"\"\"Convert title to embedding.\"\"\"\n",
    "    words = clean_text(query)\n",
    "    word_embeddings = [embeddings[word_to_idx[word]] for word in words if word in word_to_idx]\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "def text_to_embedding(text: str, word_to_idx: dict, embeddings: np.ndarray, max_len: int = 50) -> np.ndarray:\n",
    "    \"\"\"Convert text to embedding using GloVe embeddings with max pooling.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    words = text.lower().split()[:max_len]  # Limit sequence length\n",
    "    word_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            word_embeddings.append(embeddings[word_to_idx[word]])\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    # Max pooling across words\n",
    "    return np.max(word_embeddings, axis=0)\n",
    "\n",
    "def create_contrastive_dataset(df_filtered: pd.DataFrame, word_to_idx: dict, embeddings: np.ndarray, \n",
    "                              num_docs_per_sample: int = 10):\n",
    "    \"\"\"\n",
    "    Create contrastive learning dataset with positive and negative sampling.\n",
    "    \n",
    "    Steps:\n",
    "    1. Pass query through embeddings -> QUERY input\n",
    "    2. Create 10 positive documents (all targets = [1,1,1,...])\n",
    "    3. Create 10 negative documents (all targets = [0,0,0,...])\n",
    "    4. Embed documents with max pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating contrastive dataset...\")\n",
    "    \n",
    "    # Get unique queries to avoid data leakage\n",
    "    unique_queries = df_filtered.groupby('query').first().reset_index()\n",
    "    print(f\"Found {len(unique_queries)} unique queries\")\n",
    "    \n",
    "    query_embeddings = []\n",
    "    positive_doc_embeddings = []\n",
    "    negative_doc_embeddings = []\n",
    "    targets = []\n",
    "    \n",
    "    for idx, row in unique_queries.iterrows():\n",
    "        query = row['query']\n",
    "        \n",
    "        # Step 1: Embed the query\n",
    "        query_emb = text_to_embedding(query, word_to_idx, embeddings)\n",
    "        query_embeddings.append(query_emb)\n",
    "        \n",
    "        # Step 2: Create positive samples - use the query itself as positive document\n",
    "        # (In practice, you'd use actual relevant documents)\n",
    "        positive_passages = [query] * num_docs_per_sample\n",
    "        \n",
    "        # Step 3: Embed positive documents with max pooling\n",
    "        pos_doc_embs = [text_to_embedding(passage, word_to_idx, embeddings) for passage in positive_passages]\n",
    "        positive_doc_embeddings.append(pos_doc_embs)\n",
    "        \n",
    "        # Step 4: Create negative samples from other queries\n",
    "        other_queries = df_filtered[df_filtered['query'] != query]\n",
    "        negative_passages = []\n",
    "        \n",
    "        for _ in range(num_docs_per_sample):\n",
    "            if len(other_queries) > 0:\n",
    "                random_doc = other_queries.sample(n=1).iloc[0]\n",
    "                negative_passages.append(random_doc['query'])\n",
    "            else:\n",
    "                negative_passages.append(\"dummy negative text\")\n",
    "        \n",
    "        # Step 5: Embed negative documents with max pooling\n",
    "        neg_doc_embs = [text_to_embedding(passage, word_to_idx, embeddings) for passage in negative_passages]\n",
    "        negative_doc_embeddings.append(neg_doc_embs)\n",
    "        \n",
    "        # Step 6: Create targets\n",
    "        # Positive samples: all 1s [1,1,1,1,1,1,1,1,1,1]\n",
    "        # Negative samples: all 0s [0,0,0,0,0,0,0,0,0,0]\n",
    "        positive_target = [1] * num_docs_per_sample\n",
    "        negative_target = [0] * num_docs_per_sample\n",
    "        targets.append([positive_target, negative_target])\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx}/{len(unique_queries)} queries\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    query_embeddings = torch.tensor(np.array(query_embeddings), dtype=torch.float32)\n",
    "    positive_doc_embeddings = torch.tensor(np.array(positive_doc_embeddings), dtype=torch.float32)\n",
    "    negative_doc_embeddings = torch.tensor(np.array(negative_doc_embeddings), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"\\\\n✅ Dataset created successfully!\")\n",
    "    print(f\"  Query embeddings: {query_embeddings.shape}\")\n",
    "    print(f\"  Positive doc embeddings: {positive_doc_embeddings.shape}\")\n",
    "    print(f\"  Negative doc embeddings: {negative_doc_embeddings.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    print(f\"\\\\nTarget structure:\")\n",
    "    print(f\"  targets[i][0] = positive targets (all 1s): {targets[0][0]}\")\n",
    "    print(f\"  targets[i][1] = negative targets (all 0s): {targets[0][1]}\")\n",
    "    \n",
    "    return query_embeddings, positive_doc_embeddings, negative_doc_embeddings, targets\n",
    "\n",
    "# Load embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "word_to_idx, embeddings = load_glove_embeddings()\n",
    "\n",
    "if word_to_idx is None:\n",
    "    print(\"⚠️ Using dummy embeddings for testing...\")\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 100\n",
    "    word_to_idx = {f\"word_{i}\": i for i in range(vocab_size)}\n",
    "    word_to_idx.update({word: i for i, word in enumerate(['what', 'is', 'the', 'how', 'where', 'when', 'why', 'who'])})\n",
    "    embeddings = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "    print(f\"Created dummy embeddings: {embeddings.shape}\")\n",
    "else:\n",
    "    print(f\"Loaded GloVe embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Create the contrastive dataset\n",
    "query_embs, pos_doc_embs, neg_doc_embs, targets = create_contrastive_dataset(\n",
    "    df_filtered, word_to_idx, embeddings, num_docs_per_sample=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the dataset structure\n",
    "print(\"📊 Dataset Summary:\")\n",
    "print(f\"Number of queries: {len(query_embs)}\")\n",
    "print(f\"Embedding dimension: {query_embs.shape[1]}\")\n",
    "print(f\"Documents per sample: {pos_doc_embs.shape[1]}\")\n",
    "\n",
    "print(\"\\\\n🔍 Sample data:\")\n",
    "print(f\"Query embedding shape: {query_embs[0].shape}\")\n",
    "print(f\"Positive doc embeddings shape: {pos_doc_embs[0].shape}\")\n",
    "print(f\"Negative doc embeddings shape: {neg_doc_embs[0].shape}\")\n",
    "print(f\"Positive targets: {targets[0][0]}\")  # Should be [1,1,1,1,1,1,1,1,1,1]\n",
    "print(f\"Negative targets: {targets[0][1]}\")  # Should be [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "print(\"\\\\n✅ Ready for PyTorch training!\")\n",
    "print(\"Inputs:\")\n",
    "print(f\"  - query_embs: {query_embs.shape} (query embeddings)\")\n",
    "print(f\"  - pos_doc_embs: {pos_doc_embs.shape} (positive document embeddings)\")\n",
    "print(f\"  - neg_doc_embs: {neg_doc_embs.shape} (negative document embeddings)\")\n",
    "print(\"Targets:\")\n",
    "print(f\"  - targets: {targets.shape} (positive and negative labels)\")\n",
    "\n",
    "# Example of how to use in training loop\n",
    "print(\"\\\\n💡 Training loop example:\")\n",
    "print(\"for i in range(len(query_embs)):\")\n",
    "print(\"    query = query_embs[i]  # Shape: [embedding_dim]\")\n",
    "print(\"    pos_docs = pos_doc_embs[i]  # Shape: [10, embedding_dim]\")\n",
    "print(\"    neg_docs = neg_doc_embs[i]  # Shape: [10, embedding_dim]\")\n",
    "print(\"    pos_targets = targets[i][0]  # [1,1,1,1,1,1,1,1,1,1]\")\n",
    "print(\"    neg_targets = targets[i][1]  # [0,0,0,0,0,0,0,0,0,0]\")\n",
    "print(\"    # Your model training code here...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 SIMPLIFIED AND MODULAR VERSION\n",
    "import torch\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class EmbeddingProcessor:\n",
    "    \"\"\"Simple class to handle text embeddings with max pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, word_to_idx: Dict, embeddings: np.ndarray):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_dim = embeddings.shape[1]\n",
    "    \n",
    "    def embed_text(self, text: str, max_len: int = 50) -> np.ndarray:\n",
    "        \"\"\"Convert text to embedding using max pooling.\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        words = clean_text(text)[:max_len]\n",
    "        word_embeddings = [self.embeddings[self.word_to_idx[word]] \n",
    "                          for word in words if word in self.word_to_idx]\n",
    "        \n",
    "        if not word_embeddings:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "def extract_passages(df_row) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Extract positive and negative passages from a data row.\"\"\"\n",
    "    is_selected = df_row['passages.is_selected']\n",
    "    passage_texts = df_row['passages.passage_text']\n",
    "    \n",
    "    positive_passages = []\n",
    "    negative_passages = []\n",
    "    \n",
    "    if isinstance(is_selected, list) and isinstance(passage_texts, list):\n",
    "        for i, selected in enumerate(is_selected):\n",
    "            if i < len(passage_texts):\n",
    "                if selected == 1:\n",
    "                    positive_passages.append(passage_texts[i])\n",
    "                else:\n",
    "                    negative_passages.append(passage_texts[i])\n",
    "    \n",
    "    return positive_passages, negative_passages\n",
    "\n",
    "def sample_passages(passages: List[str], target_count: int, fallback_text: str = \"\") -> List[str]:\n",
    "    \"\"\"Sample exactly target_count passages, with fallback handling.\"\"\"\n",
    "    if len(passages) >= target_count:\n",
    "        return random.sample(passages, target_count)\n",
    "    \n",
    "    if not passages:\n",
    "        return [fallback_text] * target_count\n",
    "    \n",
    "    # Repeat existing passages to reach target count\n",
    "    repeated = passages * (target_count // len(passages) + 1)\n",
    "    return repeated[:target_count]\n",
    "\n",
    "def create_simple_contrastive_dataset(df_filtered: pd.DataFrame, \n",
    "                                    embedder: EmbeddingProcessor,\n",
    "                                    num_docs: int = 10) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create contrastive dataset in a simple, modular way.\n",
    "    \n",
    "    Returns:\n",
    "        query_embeddings, positive_embeddings, negative_embeddings, targets\n",
    "    \"\"\"\n",
    "    print(\"🚀 Creating simplified contrastive dataset...\")\n",
    "    \n",
    "    # Get unique queries\n",
    "    unique_queries = df_filtered.groupby('query').first().reset_index()\n",
    "    print(f\"Processing {len(unique_queries)} unique queries\")\n",
    "    \n",
    "    all_query_embs = []\n",
    "    all_pos_embs = []\n",
    "    all_neg_embs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for idx, row in unique_queries.iterrows():\n",
    "        query = row['query']\n",
    "        \n",
    "        # 1. Embed query\n",
    "        query_emb = embedder.embed_text(query)\n",
    "        all_query_embs.append(query_emb)\n",
    "        \n",
    "        # 2. Get all passages for this query\n",
    "        query_docs = df_filtered[df_filtered['query'] == query]\n",
    "        all_positive = []\n",
    "        all_negative = []\n",
    "        \n",
    "        for _, doc_row in query_docs.iterrows():\n",
    "            pos_passages, neg_passages = extract_passages(doc_row)\n",
    "            all_positive.extend(pos_passages)\n",
    "            all_negative.extend(neg_passages)\n",
    "        \n",
    "        # 3. Sample passages\n",
    "        sampled_positive = sample_passages(all_positive, num_docs, query)\n",
    "        \n",
    "        # For negatives, use passages from other queries if needed\n",
    "        if len(all_negative) < num_docs:\n",
    "            other_queries = df_filtered[df_filtered['query'] != query]\n",
    "            for _ in range(num_docs - len(all_negative)):\n",
    "                if len(other_queries) > 0:\n",
    "                    random_row = other_queries.sample(n=1).iloc[0]\n",
    "                    random_passages = random_row['passages.passage_text']\n",
    "                    if isinstance(random_passages, list) and random_passages:\n",
    "                        all_negative.append(random.choice(random_passages))\n",
    "        \n",
    "        sampled_negative = sample_passages(all_negative, num_docs, f\"negative for {query}\")\n",
    "        \n",
    "        # 4. Embed passages\n",
    "        pos_embs = [embedder.embed_text(passage) for passage in sampled_positive]\n",
    "        neg_embs = [embedder.embed_text(passage) for passage in sampled_negative]\n",
    "        \n",
    "        all_pos_embs.append(pos_embs)\n",
    "        all_neg_embs.append(neg_embs)\n",
    "        \n",
    "        # 5. Create targets\n",
    "        pos_targets = [1] * num_docs\n",
    "        neg_targets = [0] * num_docs\n",
    "        all_targets.append([pos_targets, neg_targets])\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f\"  Processed {idx}/{len(unique_queries)} queries\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    query_embeddings = torch.tensor(np.array(all_query_embs), dtype=torch.float32)\n",
    "    positive_embeddings = torch.tensor(np.array(all_pos_embs), dtype=torch.float32)\n",
    "    negative_embeddings = torch.tensor(np.array(all_neg_embs), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array(all_targets), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"✅ Dataset created!\")\n",
    "    print(f\"  Queries: {query_embeddings.shape}\")\n",
    "    print(f\"  Positive docs: {positive_embeddings.shape}\")\n",
    "    print(f\"  Negative docs: {negative_embeddings.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    \n",
    "    return query_embeddings, positive_embeddings, negative_embeddings, targets\n",
    "\n",
    "# Initialize embedder\n",
    "print(\"🔧 Setting up embeddings...\")\n",
    "word_to_idx, embeddings = load_glove_embeddings()\n",
    "\n",
    "if word_to_idx is None:\n",
    "    print(\"Using dummy embeddings...\")\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 100\n",
    "    word_to_idx = {f\"word_{i}\": i for i in range(vocab_size)}\n",
    "    # Add common words\n",
    "    common_words = ['what', 'is', 'the', 'how', 'where', 'when', 'why', 'who', 'are', 'does', 'do']\n",
    "    word_to_idx.update({word: i for i, word in enumerate(common_words)})\n",
    "    embeddings = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "\n",
    "embedder = EmbeddingProcessor(word_to_idx, embeddings)\n",
    "print(f\"Embedder ready with {embeddings.shape[1]}D embeddings\")\n",
    "\n",
    "# Create dataset\n",
    "query_embs_simple, pos_embs_simple, neg_embs_simple, targets_simple = create_simple_contrastive_dataset(\n",
    "    df_filtered, embedder, num_docs=10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
