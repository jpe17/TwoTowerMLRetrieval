{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataset to embeddings\n",
    "1. Convert MS Marco to format with query, 10 positive samples, and 10 negative samples \n",
    "2. Convert each of those to embeddings via Glove pre-trained model\n",
    "3. Prepare input torch tensors for training ML Two Towers model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Initialising functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def flatten_data(df):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        for i, passage_text in enumerate(row['passages.passage_text']):\n",
    "            rows.append({\n",
    "                'query': row['query'],\n",
    "                'query_id': row['query_id'], \n",
    "                'query_type': row['query_type'],\n",
    "                'document': passage_text,\n",
    "                'passage_sign_de': 1,\n",
    "                'passage_sign_ce': row['passages.is_selected'][i],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def add_negative_samples_with_input_id(df):\n",
    "    result_rows = []\n",
    "    input_id = 0\n",
    "    \n",
    "    # Group by query_id \n",
    "    grouped = df.groupby('query_id')\n",
    "    \n",
    "    for query_id, group in grouped:\n",
    "        # Take up to 10 positive samples from this query\n",
    "        positive_samples = group.head(10).copy()\n",
    "        \n",
    "        # Create 10 negative samples\n",
    "        negative_samples = []\n",
    "        for _ in range(10):\n",
    "            # Random query from different query_id\n",
    "            other_queries = df[df['query_id'] != query_id]\n",
    "            random_query = other_queries.sample(1).iloc[0]\n",
    "            \n",
    "            # Random passage from anywhere\n",
    "            random_passage = df['document'].sample(1).iloc[0]\n",
    "            \n",
    "            neg_sample = {\n",
    "                'query': random_query['query'],\n",
    "                'query_id': random_query['query_id'], \n",
    "                'query_type': random_query['query_type'],\n",
    "                'document': random_passage,\n",
    "                'passage_sign_de': 0,\n",
    "                'passage_sign_ce': None\n",
    "            }\n",
    "            negative_samples.append(neg_sample)\n",
    "        \n",
    "        # Add input_id to positive samples\n",
    "        for _, row in positive_samples.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            row_dict['input_id'] = input_id\n",
    "            result_rows.append(row_dict)\n",
    "            \n",
    "        # Add input_id to negative samples  \n",
    "        for neg_sample in negative_samples:\n",
    "            neg_sample['input_id'] = input_id\n",
    "            result_rows.append(neg_sample)\n",
    "            \n",
    "        input_id += 1\n",
    "    \n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "def add_negative_samples_same_query(df):\n",
    "    \"\"\"\n",
    "    Modified version where negative samples use the same query as positive samples,\n",
    "    but with non-relevant passages from other queries.\n",
    "    \"\"\"\n",
    "    result_rows = []\n",
    "    input_id = 0\n",
    "    \n",
    "    # Group by query_id \n",
    "    grouped = df.groupby('query_id')\n",
    "    \n",
    "    for query_id, group in grouped:\n",
    "        # Take up to 10 positive samples from this query\n",
    "        positive_samples = group.head(10).copy()\n",
    "        \n",
    "        # Get the query info from the first positive sample\n",
    "        first_sample = positive_samples.iloc[0]\n",
    "        query_text = first_sample['query']\n",
    "        query_type = first_sample['query_type']\n",
    "        \n",
    "        # Create 10 negative samples with SAME query but different passages\n",
    "        negative_samples = []\n",
    "        for _ in range(10):\n",
    "            # Random passage from different query_id (non-relevant passage)\n",
    "            other_queries = df[df['query_id'] != query_id]\n",
    "            random_passage = other_queries['document'].sample(1).iloc[0]\n",
    "            \n",
    "            neg_sample = {\n",
    "                'query': query_text,  # Same query as positive samples\n",
    "                'query_id': query_id,  # Same query_id as positive samples\n",
    "                'query_type': query_type,  # Same query_type as positive samples\n",
    "                'document': random_passage,  # Random non-relevant passage\n",
    "                'passage_sign_de': 0,\n",
    "                'passage_sign_ce': None\n",
    "            }\n",
    "            negative_samples.append(neg_sample)\n",
    "        \n",
    "        # Add input_id to positive samples\n",
    "        for _, row in positive_samples.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            row_dict['input_id'] = input_id\n",
    "            result_rows.append(row_dict)\n",
    "            \n",
    "        # Add input_id to negative samples  \n",
    "        for neg_sample in negative_samples:\n",
    "            neg_sample['input_id'] = input_id\n",
    "            result_rows.append(neg_sample)\n",
    "            \n",
    "        input_id += 1\n",
    "    \n",
    "    return pd.DataFrame(result_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: Get MS Marco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "ðŸ“Š Sampling 500 samples...\n",
      "Sampled: 500 samples\n"
     ]
    }
   ],
   "source": [
    "# Loading Raw\n",
    "NUMBER_OF_SAMPLES = 500\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# Load and sample data FIRST\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_parquet(\"../data/ms_marco_train.parquet\", engine='fastparquet')\n",
    "\n",
    "print(f\"ðŸ“Š Sampling {NUMBER_OF_SAMPLES:,} samples...\")\n",
    "\n",
    "df_sample = df.sample(n=NUMBER_OF_SAMPLES, random_state=42).copy()\n",
    "print(f\"Sampled: {len(df_sample)} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  answers  \\\n",
      "19000                                [No Answer Present.]   \n",
      "534732  [The first bloody violent video game is The Te...   \n",
      "792237              [$1,443 for men and $1,042 for women]   \n",
      "90616                                [No Answer Present.]   \n",
      "341997                               [No Answer Present.]   \n",
      "59538                              [Viral conjunctivitis]   \n",
      "321418                               [No Answer Present.]   \n",
      "758532                                         [$20-$350]   \n",
      "347661  [A file extension for a Structured Query Langu...   \n",
      "97092   [A sensory receptor is a structure that recogn...   \n",
      "\n",
      "                                                    query  query_id  \\\n",
      "19000   what happened to money multiplier if the reser...    665215   \n",
      "534732              what was the first violent video game    920646   \n",
      "792237                  what are personal weekly earnings    563174   \n",
      "90616                               what is dried harissa    741321   \n",
      "341997                               where do pools drain    971427   \n",
      "59538                      what causes eyes to glaze over    587770   \n",
      "321418  how to ask court to set aside judgement iowa form    343911   \n",
      "758532      how much does a graphic designer make an hour    309736   \n",
      "347661                                 what is sql format    799073   \n",
      "97092                          what are sensory receptors    564676   \n",
      "\n",
      "         query_type                                  wellFormedAnswers  \\\n",
      "19000   DESCRIPTION                                                 []   \n",
      "534732       ENTITY                                                 []   \n",
      "792237  DESCRIPTION                                                 []   \n",
      "90616   DESCRIPTION                                                 []   \n",
      "341997     LOCATION                                                 []   \n",
      "59538   DESCRIPTION                                                 []   \n",
      "321418  DESCRIPTION                                                 []   \n",
      "758532      NUMERIC                                                 []   \n",
      "347661  DESCRIPTION  [A sql format is a file extension for a Struct...   \n",
      "97092   DESCRIPTION  [A sensory receptor is a structure that recogn...   \n",
      "\n",
      "                  passages.is_selected  \\\n",
      "19000   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "534732  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
      "792237  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
      "90616   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "341997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "59538   [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
      "321418  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "758532  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
      "347661  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "97092   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "\n",
      "                                    passages.passage_text  \\\n",
      "19000   [fractional-reserve banking but not under 100-...   \n",
      "534732  [However, the incident prompted a wave of legi...   \n",
      "792237  [Personal income is an individual's total earn...   \n",
      "90616   [Authentic North African flavor! Uses: Harissa...   \n",
      "341997  [CPSC staff recommends that to ELIMINATE and n...   \n",
      "59538   [Placing a couple of Visine eye-drops in your ...   \n",
      "321418  [; and. (d) the defendant is an individual; an...   \n",
      "758532  [For example the median expected annual pay fo...   \n",
      "347661  [Extension for SQL queries. SQL is a file exte...   \n",
      "97092   [In a sensory system, a sensory receptor is a ...   \n",
      "\n",
      "                                             passages.url  \n",
      "19000   [http://www.shsu.edu/~eco_hkn/classes/ECO23405...  \n",
      "534732  [https://en.wikipedia.org/wiki/Video_game_cont...  \n",
      "792237  [https://en.wikipedia.org/wiki/Personal_income...  \n",
      "90616   [http://www.myspicesage.com/harissa-seasoning-...  \n",
      "341997  [https://apsp.org/portals/0/PDFs/Case%20Study%...  \n",
      "59538   [https://www.disabled-world.com/disability/typ...  \n",
      "321418  [https://www.justice.gov.uk/courts/procedure-r...  \n",
      "758532  [http://www1.salary.com/Web-Designer-salary.ht...  \n",
      "347661  [http://whatis.techtarget.com/fileformat/SQL-E...  \n",
      "97092   [http://psychology.wikia.com/wiki/Sensory_rece...  \n"
     ]
    }
   ],
   "source": [
    "print(df_sample.head(10))\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Transform MS Marco Dataset and include negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n",
      "After filtering: 320 samples\n",
      "                                                query  query_id   query_type  \\\n",
      "0   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "1   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "2   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "3   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "4   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "5   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "6   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "7   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "8   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "9   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "10       when is the best time to buy a flight ticket   1142634      NUMERIC   \n",
      "11                               what is soft science    797750  DESCRIPTION   \n",
      "12                 does doris day own the cypress inn    165620  DESCRIPTION   \n",
      "13  when was naval station guantÃ¡namo bay established    959054      NUMERIC   \n",
      "14                        age of majority to contract     13774      NUMERIC   \n",
      "15  which is a characteristic of the kingdom anima...   1011059  DESCRIPTION   \n",
      "16                   what type of acid is in a potato    910157       ENTITY   \n",
      "17              what are the minerals in our food for    571652       ENTITY   \n",
      "18                in what folder are favorites stored    394109  DESCRIPTION   \n",
      "19                    what  machines  does  ati  have    548932       ENTITY   \n",
      "20  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "21  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "22  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "23  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "24  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "25  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "26  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "27  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "28  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "29  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "30                           what is the written word    854719  DESCRIPTION   \n",
      "31                      how many grams equals one cup    284668      NUMERIC   \n",
      "32                 what is the average life of a bond    806468      NUMERIC   \n",
      "33                       what movie has mumbles in it    879274       ENTITY   \n",
      "34                               what is soft science    797750  DESCRIPTION   \n",
      "35            what age can puppies have peanut butter    550350      NUMERIC   \n",
      "36                  are radishes good for the stomach     24590  DESCRIPTION   \n",
      "37                temperature to bake browned brisket    512854      NUMERIC   \n",
      "38                              turtles habitat facts    525685       ENTITY   \n",
      "39           when did the battle of the midway happen    941230      NUMERIC   \n",
      "\n",
      "                                             document  passage_sign_de  \\\n",
      "0   Plasmids are extra chromosomal, double strande...                1   \n",
      "1   Answers. Relevance. Rating Newest Oldest. Best...                1   \n",
      "2   In their simplest form, plasmids require a bac...                1   \n",
      "3   Bacterial DNA â€“ a circular chromosome plus pla...                1   \n",
      "4   The bacteria may pass on the the plasmid to an...                1   \n",
      "5   Bacteria carry plasmids which is a double stra...                1   \n",
      "6   Types of Plasmids. The combination of elements...                1   \n",
      "7   You've already chosen the best response. C wou...                1   \n",
      "8   1 F-plasmids (fertility factor or sex factors)...                1   \n",
      "9   A plasmid is an extra-chromosomal element, oft...                1   \n",
      "10  The type of rayon produced is named by the pro...                0   \n",
      "11  Extension for SQL queries. SQL is a file exten...                0   \n",
      "12  Examples of monoculture include lawns and most...                0   \n",
      "13  Longchamp Singapore, a brand of elegance and l...                0   \n",
      "14  Since water is the main components of our body...                0   \n",
      "15  For example the median expected hourly pay for...                0   \n",
      "16  Cytoplasm: This internal soup of the bacterial...                0   \n",
      "17  Basal Cell Carcinoma. Basal cell carcinoma is ...                0   \n",
      "18  Tomatoes need the secondary nutrients, calcium...                0   \n",
      "19  100 miles is less than 1% of the milage most o...                0   \n",
      "20  Your doctor or endocrinologist will draw your ...                1   \n",
      "21  Sometimes, your doctor might also order a test...                1   \n",
      "22  Gravesâ€™ Disease Blood Tests. Your doctor or en...                1   \n",
      "23  TSH stimulates the production and release of T...                1   \n",
      "24  Sometimes, your doctor might also order a T4 t...                1   \n",
      "25  Thyroid hormone blood tests include: 1  Total ...                1   \n",
      "26  If your TSH level is very low, the doctor may ...                1   \n",
      "27  Your doctor may also order a T4 test. Most of ...                1   \n",
      "28  Blood tests can measure total T4, free T4, tot...                1   \n",
      "29  Most of the T4 and T3 circulates in the blood ...                1   \n",
      "30  A territory is an administrative division, usu...                0   \n",
      "31  Variant of DEVIN. It may also be partly inspir...                0   \n",
      "32  The conclusion of Sukkot marks the begining of...                0   \n",
      "33  For example the kidney secretes endocrine horm...                0   \n",
      "34  â€¢ MINERAL WAX (noun). The noun MINERAL WAX has...                0   \n",
      "35  Technically, there is no such thing as a painl...                0   \n",
      "36  This mismatch between sex and gender identity ...                0   \n",
      "37  Meaning of Barabbas. Israeli name. In Israeli,...                0   \n",
      "38  An alternative approach is the building blocks...                0   \n",
      "39  Aloe vera gel can be used as a natural conditi...                0   \n",
      "\n",
      "    passage_sign_ce  input_id  \n",
      "0               1.0         0  \n",
      "1               0.0         0  \n",
      "2               0.0         0  \n",
      "3               0.0         0  \n",
      "4               0.0         0  \n",
      "5               0.0         0  \n",
      "6               0.0         0  \n",
      "7               0.0         0  \n",
      "8               0.0         0  \n",
      "9               0.0         0  \n",
      "10              NaN         0  \n",
      "11              NaN         0  \n",
      "12              NaN         0  \n",
      "13              NaN         0  \n",
      "14              NaN         0  \n",
      "15              NaN         0  \n",
      "16              NaN         0  \n",
      "17              NaN         0  \n",
      "18              NaN         0  \n",
      "19              NaN         0  \n",
      "20              1.0         1  \n",
      "21              0.0         1  \n",
      "22              1.0         1  \n",
      "23              0.0         1  \n",
      "24              0.0         1  \n",
      "25              0.0         1  \n",
      "26              0.0         1  \n",
      "27              0.0         1  \n",
      "28              0.0         1  \n",
      "29              0.0         1  \n",
      "30              NaN         1  \n",
      "31              NaN         1  \n",
      "32              NaN         1  \n",
      "33              NaN         1  \n",
      "34              NaN         1  \n",
      "35              NaN         1  \n",
      "36              NaN         1  \n",
      "37              NaN         1  \n",
      "38              NaN         1  \n",
      "39              NaN         1  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Apply filtering AFTER sampling\n",
    "print(\"Filtering data...\")\n",
    "df_filtered = df_sample[\n",
    "         (df_sample['query'].notna()) &\n",
    "         (df_sample['query_id'].notna()) &\n",
    "         (df_sample['query_type'].notna()) &\n",
    "         (df_sample['passages.is_selected'].notna()) &\n",
    "         (df_sample['passages.is_selected'].apply(lambda x: 1 in x))\n",
    "     ].copy()\n",
    "\n",
    "print(f\"After filtering: {len(df_filtered)} samples\")\n",
    "\n",
    "# Create the wide contrastive table using simplified approach\n",
    "transformed_df = flatten_data(df_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# Use it like this:\n",
    "df_with_negatives = add_negative_samples_with_input_id(transformed_df)[:100]\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df_with_negatives.head(40))\n",
    "\n",
    "df_with_negatives.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4**: Embed all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    \"\"\"Load cached GloVe embeddings.\"\"\"\n",
    "    cache_dir = os.path.dirname(\"../data/\")\n",
    "    word_to_idx_path = os.path.join(cache_dir, \"word_to_idx.pkl\")\n",
    "    embeddings_path = os.path.join(cache_dir, \"embeddings.npy\")\n",
    "    \n",
    "    if os.path.exists(word_to_idx_path) and os.path.exists(embeddings_path):\n",
    "        with open(word_to_idx_path, 'rb') as f:\n",
    "            word_to_idx = pickle.load(f)\n",
    "        embeddings = np.load(embeddings_path)\n",
    "        return word_to_idx, embeddings\n",
    "    \n",
    "    print(\"âŒ GloVe cache not found. Please run original data processing once to create cache.\")\n",
    "    return None, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_glove_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 106\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Load embeddings\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GloVe embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m word_to_idx, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mload_glove_embeddings\u001b[49m()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš ï¸ Using dummy embeddings for testing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_glove_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "\n",
    "def text_to_embedding(text: str, word_to_idx: dict, embeddings: np.ndarray, max_len: int = 50) -> np.ndarray:\n",
    "    \"\"\"Convert text to embedding using GloVe embeddings with max pooling.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    words = text.lower().split()[:max_len]  # Limit sequence length\n",
    "    word_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            word_embeddings.append(embeddings[word_to_idx[word]])\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    # Max pooling across words\n",
    "    return np.max(word_embeddings, axis=0)\n",
    "\n",
    "def create_contrastive_dataset(df_filtered: pd.DataFrame, word_to_idx: dict, embeddings: np.ndarray, \n",
    "                              num_docs_per_sample: int = 10):\n",
    "    \"\"\"\n",
    "    Create contrastive learning dataset with positive and negative sampling.\n",
    "    \n",
    "    Steps:\n",
    "    1. Pass query through embeddings -> QUERY input\n",
    "    2. Create 10 positive documents (all targets = [1,1,1,...])\n",
    "    3. Create 10 negative documents (all targets = [0,0,0,...])\n",
    "    4. Embed documents with max pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating contrastive dataset...\")\n",
    "    \n",
    "    # Get unique queries to avoid data leakage\n",
    "    unique_queries = df_filtered.groupby('query').first().reset_index()\n",
    "    print(f\"Found {len(unique_queries)} unique queries\")\n",
    "    \n",
    "    query_embeddings = []\n",
    "    positive_doc_embeddings = []\n",
    "    negative_doc_embeddings = []\n",
    "    targets = []\n",
    "    \n",
    "    for idx, row in unique_queries.iterrows():\n",
    "        query = row['query']\n",
    "        \n",
    "        # Step 1: Embed the query\n",
    "        query_emb = text_to_embedding(query, word_to_idx, embeddings)\n",
    "        query_embeddings.append(query_emb)\n",
    "        \n",
    "        # Step 2: Create positive samples - use the query itself as positive document\n",
    "        # (In practice, you'd use actual relevant documents)\n",
    "        positive_passages = [query] * num_docs_per_sample\n",
    "        \n",
    "        # Step 3: Embed positive documents with max pooling\n",
    "        pos_doc_embs = [text_to_embedding(passage, word_to_idx, embeddings) for passage in positive_passages]\n",
    "        positive_doc_embeddings.append(pos_doc_embs)\n",
    "        \n",
    "        # Step 4: Create negative samples from other queries\n",
    "        other_queries = df_filtered[df_filtered['query'] != query]\n",
    "        negative_passages = []\n",
    "        \n",
    "        for _ in range(num_docs_per_sample):\n",
    "            if len(other_queries) > 0:\n",
    "                random_doc = other_queries.sample(n=1).iloc[0]\n",
    "                negative_passages.append(random_doc['query'])\n",
    "            else:\n",
    "                negative_passages.append(\"dummy negative text\")\n",
    "        \n",
    "        # Step 5: Embed negative documents with max pooling\n",
    "        neg_doc_embs = [text_to_embedding(passage, word_to_idx, embeddings) for passage in negative_passages]\n",
    "        negative_doc_embeddings.append(neg_doc_embs)\n",
    "        \n",
    "        # Step 6: Create targets\n",
    "        # Positive samples: all 1s [1,1,1,1,1,1,1,1,1,1]\n",
    "        # Negative samples: all 0s [0,0,0,0,0,0,0,0,0,0]\n",
    "        positive_target = [1] * num_docs_per_sample\n",
    "        negative_target = [0] * num_docs_per_sample\n",
    "        targets.append([positive_target, negative_target])\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx}/{len(unique_queries)} queries\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    query_embeddings = torch.tensor(np.array(query_embeddings), dtype=torch.float32)\n",
    "    positive_doc_embeddings = torch.tensor(np.array(positive_doc_embeddings), dtype=torch.float32)\n",
    "    negative_doc_embeddings = torch.tensor(np.array(negative_doc_embeddings), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"\\\\nâœ… Dataset created successfully!\")\n",
    "    print(f\"  Query embeddings: {query_embeddings.shape}\")\n",
    "    print(f\"  Positive doc embeddings: {positive_doc_embeddings.shape}\")\n",
    "    print(f\"  Negative doc embeddings: {negative_doc_embeddings.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    print(f\"\\\\nTarget structure:\")\n",
    "    print(f\"  targets[i][0] = positive targets (all 1s): {targets[0][0]}\")\n",
    "    print(f\"  targets[i][1] = negative targets (all 0s): {targets[0][1]}\")\n",
    "    \n",
    "    return query_embeddings, positive_doc_embeddings, negative_doc_embeddings, targets\n",
    "\n",
    "# Load embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "word_to_idx, embeddings = load_glove_embeddings()\n",
    "\n",
    "if word_to_idx is None:\n",
    "    print(\"âš ï¸ Using dummy embeddings for testing...\")\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 100\n",
    "    word_to_idx = {f\"word_{i}\": i for i in range(vocab_size)}\n",
    "    word_to_idx.update({word: i for i, word in enumerate(['what', 'is', 'the', 'how', 'where', 'when', 'why', 'who'])})\n",
    "    embeddings = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "    print(f\"Created dummy embeddings: {embeddings.shape}\")\n",
    "else:\n",
    "    print(f\"Loaded GloVe embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Create the contrastive dataset\n",
    "query_embs, pos_doc_embs, neg_doc_embs, targets = create_contrastive_dataset(\n",
    "    df_filtered, word_to_idx, embeddings, num_docs_per_sample=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the dataset structure\n",
    "print(\"ðŸ“Š Dataset Summary:\")\n",
    "print(f\"Number of queries: {len(query_embs)}\")\n",
    "print(f\"Embedding dimension: {query_embs.shape[1]}\")\n",
    "print(f\"Documents per sample: {pos_doc_embs.shape[1]}\")\n",
    "\n",
    "print(\"\\\\nðŸ” Sample data:\")\n",
    "print(f\"Query embedding shape: {query_embs[0].shape}\")\n",
    "print(f\"Positive doc embeddings shape: {pos_doc_embs[0].shape}\")\n",
    "print(f\"Negative doc embeddings shape: {neg_doc_embs[0].shape}\")\n",
    "print(f\"Positive targets: {targets[0][0]}\")  # Should be [1,1,1,1,1,1,1,1,1,1]\n",
    "print(f\"Negative targets: {targets[0][1]}\")  # Should be [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "print(\"\\\\nâœ… Ready for PyTorch training!\")\n",
    "print(\"Inputs:\")\n",
    "print(f\"  - query_embs: {query_embs.shape} (query embeddings)\")\n",
    "print(f\"  - pos_doc_embs: {pos_doc_embs.shape} (positive document embeddings)\")\n",
    "print(f\"  - neg_doc_embs: {neg_doc_embs.shape} (negative document embeddings)\")\n",
    "print(\"Targets:\")\n",
    "print(f\"  - targets: {targets.shape} (positive and negative labels)\")\n",
    "\n",
    "# Example of how to use in training loop\n",
    "print(\"\\\\nðŸ’¡ Training loop example:\")\n",
    "print(\"for i in range(len(query_embs)):\")\n",
    "print(\"    query = query_embs[i]  # Shape: [embedding_dim]\")\n",
    "print(\"    pos_docs = pos_doc_embs[i]  # Shape: [10, embedding_dim]\")\n",
    "print(\"    neg_docs = neg_doc_embs[i]  # Shape: [10, embedding_dim]\")\n",
    "print(\"    pos_targets = targets[i][0]  # [1,1,1,1,1,1,1,1,1,1]\")\n",
    "print(\"    neg_targets = targets[i][1]  # [0,0,0,0,0,0,0,0,0,0]\")\n",
    "print(\"    # Your model training code here...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ SIMPLIFIED AND MODULAR VERSION\n",
    "import torch\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class EmbeddingProcessor:\n",
    "    \"\"\"Simple class to handle text embeddings with max pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, word_to_idx: Dict, embeddings: np.ndarray):\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_dim = embeddings.shape[1]\n",
    "    \n",
    "    def embed_text(self, text: str, max_len: int = 50) -> np.ndarray:\n",
    "        \"\"\"Convert text to embedding using max pooling.\"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        words = clean_text(text)[:max_len]\n",
    "        word_embeddings = [self.embeddings[self.word_to_idx[word]] \n",
    "                          for word in words if word in self.word_to_idx]\n",
    "        \n",
    "        if not word_embeddings:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "def extract_passages(df_row) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Extract positive and negative passages from a data row.\"\"\"\n",
    "    is_selected = df_row['passages.is_selected']\n",
    "    passage_texts = df_row['passages.passage_text']\n",
    "    \n",
    "    positive_passages = []\n",
    "    negative_passages = []\n",
    "    \n",
    "    if isinstance(is_selected, list) and isinstance(passage_texts, list):\n",
    "        for i, selected in enumerate(is_selected):\n",
    "            if i < len(passage_texts):\n",
    "                if selected == 1:\n",
    "                    positive_passages.append(passage_texts[i])\n",
    "                else:\n",
    "                    negative_passages.append(passage_texts[i])\n",
    "    \n",
    "    return positive_passages, negative_passages\n",
    "\n",
    "def sample_passages(passages: List[str], target_count: int, fallback_text: str = \"\") -> List[str]:\n",
    "    \"\"\"Sample exactly target_count passages, with fallback handling.\"\"\"\n",
    "    if len(passages) >= target_count:\n",
    "        return random.sample(passages, target_count)\n",
    "    \n",
    "    if not passages:\n",
    "        return [fallback_text] * target_count\n",
    "    \n",
    "    # Repeat existing passages to reach target count\n",
    "    repeated = passages * (target_count // len(passages) + 1)\n",
    "    return repeated[:target_count]\n",
    "\n",
    "def create_simple_contrastive_dataset(df_filtered: pd.DataFrame, \n",
    "                                    embedder: EmbeddingProcessor,\n",
    "                                    num_docs: int = 10) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create contrastive dataset in a simple, modular way.\n",
    "    \n",
    "    Returns:\n",
    "        query_embeddings, positive_embeddings, negative_embeddings, targets\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Creating simplified contrastive dataset...\")\n",
    "    \n",
    "    # Get unique queries\n",
    "    unique_queries = df_filtered.groupby('query').first().reset_index()\n",
    "    print(f\"Processing {len(unique_queries)} unique queries\")\n",
    "    \n",
    "    all_query_embs = []\n",
    "    all_pos_embs = []\n",
    "    all_neg_embs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for idx, row in unique_queries.iterrows():\n",
    "        query = row['query']\n",
    "        \n",
    "        # 1. Embed query\n",
    "        query_emb = embedder.embed_text(query)\n",
    "        all_query_embs.append(query_emb)\n",
    "        \n",
    "        # 2. Get all passages for this query\n",
    "        query_docs = df_filtered[df_filtered['query'] == query]\n",
    "        all_positive = []\n",
    "        all_negative = []\n",
    "        \n",
    "        for _, doc_row in query_docs.iterrows():\n",
    "            pos_passages, neg_passages = extract_passages(doc_row)\n",
    "            all_positive.extend(pos_passages)\n",
    "            all_negative.extend(neg_passages)\n",
    "        \n",
    "        # 3. Sample passages\n",
    "        sampled_positive = sample_passages(all_positive, num_docs, query)\n",
    "        \n",
    "        # For negatives, use passages from other queries if needed\n",
    "        if len(all_negative) < num_docs:\n",
    "            other_queries = df_filtered[df_filtered['query'] != query]\n",
    "            for _ in range(num_docs - len(all_negative)):\n",
    "                if len(other_queries) > 0:\n",
    "                    random_row = other_queries.sample(n=1).iloc[0]\n",
    "                    random_passages = random_row['passages.passage_text']\n",
    "                    if isinstance(random_passages, list) and random_passages:\n",
    "                        all_negative.append(random.choice(random_passages))\n",
    "        \n",
    "        sampled_negative = sample_passages(all_negative, num_docs, f\"negative for {query}\")\n",
    "        \n",
    "        # 4. Embed passages\n",
    "        pos_embs = [embedder.embed_text(passage) for passage in sampled_positive]\n",
    "        neg_embs = [embedder.embed_text(passage) for passage in sampled_negative]\n",
    "        \n",
    "        all_pos_embs.append(pos_embs)\n",
    "        all_neg_embs.append(neg_embs)\n",
    "        \n",
    "        # 5. Create targets\n",
    "        pos_targets = [1] * num_docs\n",
    "        neg_targets = [0] * num_docs\n",
    "        all_targets.append([pos_targets, neg_targets])\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f\"  Processed {idx}/{len(unique_queries)} queries\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    query_embeddings = torch.tensor(np.array(all_query_embs), dtype=torch.float32)\n",
    "    positive_embeddings = torch.tensor(np.array(all_pos_embs), dtype=torch.float32)\n",
    "    negative_embeddings = torch.tensor(np.array(all_neg_embs), dtype=torch.float32)\n",
    "    targets = torch.tensor(np.array(all_targets), dtype=torch.float32)\n",
    "    \n",
    "    print(f\"âœ… Dataset created!\")\n",
    "    print(f\"  Queries: {query_embeddings.shape}\")\n",
    "    print(f\"  Positive docs: {positive_embeddings.shape}\")\n",
    "    print(f\"  Negative docs: {negative_embeddings.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    \n",
    "    return query_embeddings, positive_embeddings, negative_embeddings, targets\n",
    "\n",
    "# Initialize embedder\n",
    "print(\"ðŸ”§ Setting up embeddings...\")\n",
    "word_to_idx, embeddings = load_glove_embeddings()\n",
    "\n",
    "if word_to_idx is None:\n",
    "    print(\"Using dummy embeddings...\")\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 100\n",
    "    word_to_idx = {f\"word_{i}\": i for i in range(vocab_size)}\n",
    "    # Add common words\n",
    "    common_words = ['what', 'is', 'the', 'how', 'where', 'when', 'why', 'who', 'are', 'does', 'do']\n",
    "    word_to_idx.update({word: i for i, word in enumerate(common_words)})\n",
    "    embeddings = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "\n",
    "embedder = EmbeddingProcessor(word_to_idx, embeddings)\n",
    "print(f\"Embedder ready with {embeddings.shape[1]}D embeddings\")\n",
    "\n",
    "# Create dataset\n",
    "query_embs_simple, pos_embs_simple, neg_embs_simple, targets_simple = create_simple_contrastive_dataset(\n",
    "    df_filtered, embedder, num_docs=10\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
