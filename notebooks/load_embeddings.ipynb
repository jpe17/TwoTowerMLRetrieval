{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Dataset to embeddings\n",
    "1. Convert MS Marco to format with query, 10 positive samples, and 10 negative samples \n",
    "2. Convert each of those to embeddings via Glove pre-trained model\n",
    "3. Prepare input torch tensors for training ML Two Towers model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Initialising functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def flatten_data(df):\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        for i, passage_text in enumerate(row['passages.passage_text']):\n",
    "            rows.append({\n",
    "                'query': row['query'],\n",
    "                'query_id': row['query_id'], \n",
    "                'query_type': row['query_type'],\n",
    "                'document': passage_text,\n",
    "                'passage_sign_de': 1,\n",
    "                'passage_sign_ce': row['passages.is_selected'][i],\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def add_negative_samples_with_input_id(df):\n",
    "    result_rows = []\n",
    "    input_id = 0\n",
    "    \n",
    "    # Group by query_id \n",
    "    grouped = df.groupby('query_id')\n",
    "    \n",
    "    for query_id, group in grouped:\n",
    "        # Take up to 10 positive samples from this query\n",
    "        positive_samples = group.head(10).copy()\n",
    "        \n",
    "        # Create 10 negative samples\n",
    "        negative_samples = []\n",
    "        for _ in range(10):\n",
    "            # Random query from different query_id\n",
    "            other_queries = df[df['query_id'] != query_id]\n",
    "            random_query = other_queries.sample(1).iloc[0]\n",
    "            \n",
    "            # Random passage from anywhere\n",
    "            random_passage = df['document'].sample(1).iloc[0]\n",
    "            \n",
    "            neg_sample = {\n",
    "                'query': random_query['query'],\n",
    "                'query_id': random_query['query_id'], \n",
    "                'query_type': random_query['query_type'],\n",
    "                'document': random_passage,\n",
    "                'passage_sign_de': 0,\n",
    "                'passage_sign_ce': None\n",
    "            }\n",
    "            negative_samples.append(neg_sample)\n",
    "        \n",
    "        # Add input_id to positive samples\n",
    "        for _, row in positive_samples.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            row_dict['input_id'] = input_id\n",
    "            result_rows.append(row_dict)\n",
    "            \n",
    "        # Add input_id to negative samples  \n",
    "        for neg_sample in negative_samples:\n",
    "            neg_sample['input_id'] = input_id\n",
    "            result_rows.append(neg_sample)\n",
    "            \n",
    "        input_id += 1\n",
    "    \n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "def add_negative_samples_same_query(df):\n",
    "    \"\"\"\n",
    "    Modified version where negative samples use the same query as positive samples,\n",
    "    but with non-relevant passages from other queries.\n",
    "    \"\"\"\n",
    "    result_rows = []\n",
    "    input_id = 0\n",
    "    \n",
    "    # Group by query_id \n",
    "    grouped = df.groupby('query_id')\n",
    "    \n",
    "    for query_id, group in grouped:\n",
    "        # Take up to 10 positive samples from this query\n",
    "        positive_samples = group.head(10).copy()\n",
    "        \n",
    "        # Get the query info from the first positive sample\n",
    "        first_sample = positive_samples.iloc[0]\n",
    "        query_text = first_sample['query']\n",
    "        query_type = first_sample['query_type']\n",
    "        \n",
    "        # Create 10 negative samples with SAME query but different passages\n",
    "        negative_samples = []\n",
    "        for _ in range(10):\n",
    "            # Random passage from different query_id (non-relevant passage)\n",
    "            other_queries = df[df['query_id'] != query_id]\n",
    "            random_passage = other_queries['document'].sample(1).iloc[0]\n",
    "            \n",
    "            neg_sample = {\n",
    "                'query': query_text,  # Same query as positive samples\n",
    "                'query_id': query_id,  # Same query_id as positive samples\n",
    "                'query_type': query_type,  # Same query_type as positive samples\n",
    "                'document': random_passage,  # Random non-relevant passage\n",
    "                'passage_sign_de': 0,\n",
    "                'passage_sign_ce': None\n",
    "            }\n",
    "            negative_samples.append(neg_sample)\n",
    "        \n",
    "        # Add input_id to positive samples\n",
    "        for _, row in positive_samples.iterrows():\n",
    "            row_dict = row.to_dict()\n",
    "            row_dict['input_id'] = input_id\n",
    "            result_rows.append(row_dict)\n",
    "            \n",
    "        # Add input_id to negative samples  \n",
    "        for neg_sample in negative_samples:\n",
    "            neg_sample['input_id'] = input_id\n",
    "            result_rows.append(neg_sample)\n",
    "            \n",
    "        input_id += 1\n",
    "    \n",
    "    return pd.DataFrame(result_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: Get MS Marco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "ðŸ“Š Sampling 500 samples...\n",
      "Sampled: 500 samples\n"
     ]
    }
   ],
   "source": [
    "# Loading Raw\n",
    "NUMBER_OF_SAMPLES = 500\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# Load and sample data FIRST\n",
    "print(\"Loading raw data...\")\n",
    "df = pd.read_parquet(\"../data/ms_marco_train.parquet\", engine='fastparquet')\n",
    "\n",
    "print(f\"ðŸ“Š Sampling {NUMBER_OF_SAMPLES:,} samples...\")\n",
    "\n",
    "df_sample = df.sample(n=NUMBER_OF_SAMPLES, random_state=42).copy()\n",
    "print(f\"Sampled: {len(df_sample)} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Transform MS Marco Dataset and include negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n",
      "After filtering: 320 samples\n",
      "                                                query  query_id   query_type  \\\n",
      "0   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "1   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "2   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "3   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "4   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "5   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "6   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "7   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "8   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "9   Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "10  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "11  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "12  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "13  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "14  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "15  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "16  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "17  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "18  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "19  Determine the significance of plasmids for bac...      2718  DESCRIPTION   \n",
      "20  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "21  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "22  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "23  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "24  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "25  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "26  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "27  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "28  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "29  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "30  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "31  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "32  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "33  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "34  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "35  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "36  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "37  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "38  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "39  When doctor orders blood test of T4... what is T4      7975      NUMERIC   \n",
      "\n",
      "                                             document  passage_sign_de  \\\n",
      "0   Plasmids are extra chromosomal, double strande...                1   \n",
      "1   Answers. Relevance. Rating Newest Oldest. Best...                1   \n",
      "2   In their simplest form, plasmids require a bac...                1   \n",
      "3   Bacterial DNA â€“ a circular chromosome plus pla...                1   \n",
      "4   The bacteria may pass on the the plasmid to an...                1   \n",
      "5   Bacteria carry plasmids which is a double stra...                1   \n",
      "6   Types of Plasmids. The combination of elements...                1   \n",
      "7   You've already chosen the best response. C wou...                1   \n",
      "8   1 F-plasmids (fertility factor or sex factors)...                1   \n",
      "9   A plasmid is an extra-chromosomal element, oft...                1   \n",
      "10  This article is about the musician. For the au...                0   \n",
      "11  I hear that as part of the deal for Viceland, ...                0   \n",
      "12  Wabi-sabi tea bowl, Azuchi-Momoyama period, 16...                0   \n",
      "13  In parts of Eastern Andalusia, a much thinner ...                0   \n",
      "14  Great, your website is structured using HTML h...                0   \n",
      "15  Pancreatitis is inflammation of the pancreas, ...                0   \n",
      "16  North America. 1  United States. 2  Canada. 3 ...                0   \n",
      "17  For example: 1  Basque, a regional language in...                0   \n",
      "18  The High Pass Filter Circuit. In this circuit ...                0   \n",
      "19  According to the United States Department of A...                0   \n",
      "20  Your doctor or endocrinologist will draw your ...                1   \n",
      "21  Sometimes, your doctor might also order a test...                1   \n",
      "22  Gravesâ€™ Disease Blood Tests. Your doctor or en...                1   \n",
      "23  TSH stimulates the production and release of T...                1   \n",
      "24  Sometimes, your doctor might also order a T4 t...                1   \n",
      "25  Thyroid hormone blood tests include: 1  Total ...                1   \n",
      "26  If your TSH level is very low, the doctor may ...                1   \n",
      "27  Your doctor may also order a T4 test. Most of ...                1   \n",
      "28  Blood tests can measure total T4, free T4, tot...                1   \n",
      "29  Most of the T4 and T3 circulates in the blood ...                1   \n",
      "30  use philanthropy in a sentence you couldn t ex...                0   \n",
      "31  In the morning, as they went along, they saw t...                0   \n",
      "32  When the first gag rule was instituted in 1836...                0   \n",
      "33  Some people tell me if they're too small, they...                0   \n",
      "34  By setting the clock rate to a long enough per...                0   \n",
      "35  â€œBut we are looking at ways for the government...                0   \n",
      "36  Yakima War. The Yakima War (1855-1858) was a c...                0   \n",
      "37  Early life and work with Wilco [edit]. Jay Ben...                0   \n",
      "38  1 In many cases, genetic testing is used to co...                0   \n",
      "39  This method is much the same as a bain-marie e...                0   \n",
      "\n",
      "    passage_sign_ce  input_id  \n",
      "0               1.0         0  \n",
      "1               0.0         0  \n",
      "2               0.0         0  \n",
      "3               0.0         0  \n",
      "4               0.0         0  \n",
      "5               0.0         0  \n",
      "6               0.0         0  \n",
      "7               0.0         0  \n",
      "8               0.0         0  \n",
      "9               0.0         0  \n",
      "10              NaN         0  \n",
      "11              NaN         0  \n",
      "12              NaN         0  \n",
      "13              NaN         0  \n",
      "14              NaN         0  \n",
      "15              NaN         0  \n",
      "16              NaN         0  \n",
      "17              NaN         0  \n",
      "18              NaN         0  \n",
      "19              NaN         0  \n",
      "20              1.0         1  \n",
      "21              0.0         1  \n",
      "22              1.0         1  \n",
      "23              0.0         1  \n",
      "24              0.0         1  \n",
      "25              0.0         1  \n",
      "26              0.0         1  \n",
      "27              0.0         1  \n",
      "28              0.0         1  \n",
      "29              0.0         1  \n",
      "30              NaN         1  \n",
      "31              NaN         1  \n",
      "32              NaN         1  \n",
      "33              NaN         1  \n",
      "34              NaN         1  \n",
      "35              NaN         1  \n",
      "36              NaN         1  \n",
      "37              NaN         1  \n",
      "38              NaN         1  \n",
      "39              NaN         1  \n"
     ]
    }
   ],
   "source": [
    "# Apply filtering AFTER sampling\n",
    "print(\"Filtering data...\")\n",
    "df_filtered = df_sample[\n",
    "         (df_sample['query'].notna()) &\n",
    "         (df_sample['query_id'].notna()) &\n",
    "         (df_sample['query_type'].notna()) &\n",
    "         (df_sample['passages.is_selected'].notna()) &\n",
    "         (df_sample['passages.is_selected'].apply(lambda x: 1 in x))\n",
    "     ].copy()\n",
    "\n",
    "print(f\"After filtering: {len(df_filtered)} samples\")\n",
    "\n",
    "# Create the wide contrastive table using simplified approach\n",
    "transformed_df = flatten_data(df_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# Use it like this:\n",
    "df_with_negatives = add_negative_samples_with_input_id(transformed_df)\n",
    "df_final = add_negative_samples_same_query(df_with_negatives)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df_final.head(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4**: Embed all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def load_glove_embeddings():\n",
    "    \"\"\"Load cached GloVe embeddings.\"\"\"\n",
    "    cache_dir = os.path.dirname(\"../data/\")\n",
    "    word_to_idx_path = os.path.join(cache_dir, \"word_to_idx.pkl\")\n",
    "    embeddings_path = os.path.join(cache_dir, \"embeddings.npy\")\n",
    "    \n",
    "    if os.path.exists(word_to_idx_path) and os.path.exists(embeddings_path):\n",
    "        with open(word_to_idx_path, 'rb') as f:\n",
    "            word_to_idx = pickle.load(f)\n",
    "        embeddings = np.load(embeddings_path)\n",
    "        return word_to_idx, embeddings\n",
    "    \n",
    "    print(\"âŒ GloVe cache not found. Please run original data processing once to create cache.\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and tokenize text.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', str(text).lower())\n",
    "    return text.split()\n",
    "\n",
    "def text_to_embedding(text, word_to_idx, embeddings, max_len=50):\n",
    "    \"\"\"Convert text to embedding using GloVe embeddings with max pooling.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    words = clean_text(text)[:max_len]  # Limit sequence length\n",
    "    word_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            word_embeddings.append(embeddings[word_to_idx[word]])\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(embeddings.shape[1])\n",
    "    \n",
    "    # Max pooling across words\n",
    "    return np.max(word_embeddings, axis=0)\n",
    "\n",
    "def embed_dataframe(df, word_to_idx, embeddings):\n",
    "    \"\"\"Embed query and document columns in the dataframe.\"\"\"\n",
    "    print(f\"ðŸ”„ Embedding {len(df)} rows...\")\n",
    "    \n",
    "    # Embed queries\n",
    "    print(\"Embedding queries...\")\n",
    "    query_embeddings = []\n",
    "    for i, query in enumerate(df['query']):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  Query {i}/{len(df)}\")\n",
    "        query_emb = text_to_embedding(query, word_to_idx, embeddings)\n",
    "        query_embeddings.append(query_emb)\n",
    "    \n",
    "    # Embed documents\n",
    "    print(\"Embedding documents...\")\n",
    "    document_embeddings = []\n",
    "    for i, document in enumerate(df['document']):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"  Document {i}/{len(df)}\")\n",
    "        doc_emb = text_to_embedding(document, word_to_idx, embeddings)\n",
    "        document_embeddings.append(doc_emb)\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    df_embedded = df.copy()\n",
    "    df_embedded['query_embedding'] = query_embeddings\n",
    "    df_embedded['document_embedding'] = document_embeddings\n",
    "    \n",
    "    print(\"âœ… Embedding complete!\")\n",
    "    return df_embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading GloVe embeddings...\n",
      "âœ… Successfully loaded GloVe embeddings: (400000, 200)\n",
      "\\nðŸš€ Starting embedding process...\n",
      "ðŸ”„ Embedding 6400 rows...\n",
      "Embedding queries...\n",
      "  Query 0/6400\n",
      "  Query 1000/6400\n",
      "  Query 2000/6400\n",
      "  Query 3000/6400\n",
      "  Query 4000/6400\n",
      "  Query 5000/6400\n",
      "  Query 6000/6400\n",
      "Embedding documents...\n",
      "  Document 0/6400\n",
      "  Document 1000/6400\n",
      "  Document 2000/6400\n",
      "  Document 3000/6400\n",
      "  Document 4000/6400\n",
      "  Document 5000/6400\n",
      "  Document 6000/6400\n",
      "âœ… Embedding complete!\n",
      "\\nðŸ“Š Results:\n",
      "Original dataframe shape: (6400, 7)\n",
      "Embedded dataframe shape: (6400, 9)\n",
      "Embedding dimension: 200\n",
      "\\nðŸ” Sample embeddings:\n",
      "First query: 'Determine the significance of plasmids for bacteri...'\n",
      "Query embedding shape: (200,)\n",
      "Document embedding shape: (200,)\n",
      "Query embedding preview: [0.45524 0.86093 0.45565 0.59798 0.22503]\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "print(\"ðŸ“¥ Loading GloVe embeddings...\")\n",
    "word_to_idx, embeddings = load_glove_embeddings()\n",
    "\n",
    "# If no cached embeddings found, create dummy ones for testing\n",
    "if word_to_idx is None:\n",
    "    print(\"âš ï¸ Creating dummy embeddings for testing...\")\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 200  # Match your EMBEDDING_DIM\n",
    "    \n",
    "    # Create dummy vocabulary with common words\n",
    "    common_words = ['what', 'is', 'the', 'how', 'where', 'when', 'why', 'who', 'are', 'does', 'do', \n",
    "                   'this', 'that', 'these', 'those', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at',\n",
    "                   'for', 'with', 'by', 'from', 'to', 'of', 'about', 'blood', 'test', 'doctor', 'cell',\n",
    "                   'bacteria', 'plasmid', 'medical', 'health', 'significance', 'determine']\n",
    "    \n",
    "    word_to_idx = {}\n",
    "    for i, word in enumerate(common_words):\n",
    "        word_to_idx[word] = i\n",
    "    \n",
    "    # Fill remaining slots with dummy words\n",
    "    for i in range(len(common_words), vocab_size):\n",
    "        word_to_idx[f\"word_{i}\"] = i\n",
    "    \n",
    "    # Create random embeddings\n",
    "    embeddings = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "    print(f\"Created dummy embeddings: {embeddings.shape}\")\n",
    "else:\n",
    "    print(f\"âœ… Successfully loaded GloVe embeddings: {embeddings.shape}\")\n",
    "\n",
    "# Embed the df_final dataframe\n",
    "print(\"\\\\nðŸš€ Starting embedding process...\")\n",
    "df_embedded = embed_dataframe(df_final, word_to_idx, embeddings)\n",
    "\n",
    "print(f\"\\\\nðŸ“Š Results:\")\n",
    "print(f\"Original dataframe shape: {df_final.shape}\")\n",
    "print(f\"Embedded dataframe shape: {df_embedded.shape}\")\n",
    "print(f\"Embedding dimension: {len(df_embedded['query_embedding'].iloc[0])}\")\n",
    "\n",
    "# Show sample embeddings\n",
    "print(\"\\\\nðŸ” Sample embeddings:\")\n",
    "print(f\"First query: '{df_embedded['query'].iloc[0][:50]}...'\")\n",
    "print(f\"Query embedding shape: {df_embedded['query_embedding'].iloc[0].shape}\")\n",
    "print(f\"Document embedding shape: {df_embedded['document_embedding'].iloc[0].shape}\")\n",
    "print(f\"Query embedding preview: {df_embedded['query_embedding'].iloc[0][:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Organizing data for Two Tower model training...\n",
      "\\nâœ… PyTorch tensors ready for Two Tower training!\n",
      "Query embeddings: torch.Size([284, 200])\n",
      "Positive document embeddings: torch.Size([284, 10, 200])\n",
      "Negative document embeddings: torch.Size([284, 10, 200])\n",
      "Targets: torch.Size([284, 2, 10])\n",
      "\\nðŸŽ¯ Training data structure:\n",
      "- 284 training samples\n",
      "- Each query has 10 positive + 10 negative documents\n",
      "- Embedding dimension: 200\n",
      "- Positive targets: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "- Negative targets: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\\nðŸ’¾ Saving tensors...\n",
      "âœ… Saved to '../data/embedded_training_data.pt'\n",
      "\\nðŸš€ Ready for Two Tower model training!\n"
     ]
    }
   ],
   "source": [
    "# Organize embedded data for Two Tower training\n",
    "print(\"ðŸ”§ Organizing data for Two Tower model training...\")\n",
    "\n",
    "# Group by input_id to create query-document pairs\n",
    "grouped_data = df_embedded.groupby('input_id')\n",
    "\n",
    "query_embeddings_list = []\n",
    "positive_doc_embeddings_list = []\n",
    "negative_doc_embeddings_list = []\n",
    "targets_list = []\n",
    "\n",
    "for input_id, group in grouped_data:\n",
    "    # Get positive samples (passage_sign_de = 1)\n",
    "    positive_samples = group[group['passage_sign_de'] == 1]\n",
    "    # Get negative samples (passage_sign_de = 0) \n",
    "    negative_samples = group[group['passage_sign_de'] == 0]\n",
    "    \n",
    "    if len(positive_samples) > 0 and len(negative_samples) > 0:\n",
    "        # Use the query from the first positive sample\n",
    "        query_emb = positive_samples['query_embedding'].iloc[0]\n",
    "        query_embeddings_list.append(query_emb)\n",
    "        \n",
    "        # Get positive document embeddings (limit to 10)\n",
    "        pos_doc_embs = positive_samples['document_embedding'].tolist()[:10]\n",
    "        # Pad with zeros if less than 10\n",
    "        while len(pos_doc_embs) < 10:\n",
    "            pos_doc_embs.append(np.zeros_like(pos_doc_embs[0] if pos_doc_embs else query_emb))\n",
    "        positive_doc_embeddings_list.append(pos_doc_embs[:10])\n",
    "        \n",
    "        # Get negative document embeddings (limit to 10)\n",
    "        neg_doc_embs = negative_samples['document_embedding'].tolist()[:10]\n",
    "        # Pad with zeros if less than 10\n",
    "        while len(neg_doc_embs) < 10:\n",
    "            neg_doc_embs.append(np.zeros_like(neg_doc_embs[0] if neg_doc_embs else query_emb))\n",
    "        negative_doc_embeddings_list.append(neg_doc_embs[:10])\n",
    "        \n",
    "        # Create targets: [positive_targets, negative_targets]\n",
    "        pos_targets = [1] * 10\n",
    "        neg_targets = [0] * 10\n",
    "        targets_list.append([pos_targets, neg_targets])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "query_embeddings_tensor = torch.tensor(np.array(query_embeddings_list), dtype=torch.float32)\n",
    "positive_embeddings_tensor = torch.tensor(np.array(positive_doc_embeddings_list), dtype=torch.float32)\n",
    "negative_embeddings_tensor = torch.tensor(np.array(negative_doc_embeddings_list), dtype=torch.float32)\n",
    "targets_tensor = torch.tensor(np.array(targets_list), dtype=torch.float32)\n",
    "\n",
    "print(\"\\\\nâœ… PyTorch tensors ready for Two Tower training!\")\n",
    "print(f\"Query embeddings: {query_embeddings_tensor.shape}\")\n",
    "print(f\"Positive document embeddings: {positive_embeddings_tensor.shape}\")\n",
    "print(f\"Negative document embeddings: {negative_embeddings_tensor.shape}\")\n",
    "print(f\"Targets: {targets_tensor.shape}\")\n",
    "\n",
    "print(\"\\\\nðŸŽ¯ Training data structure:\")\n",
    "print(f\"- {len(query_embeddings_tensor)} training samples\")\n",
    "print(f\"- Each query has 10 positive + 10 negative documents\")  \n",
    "print(f\"- Embedding dimension: {query_embeddings_tensor.shape[1]}\")\n",
    "print(f\"- Positive targets: {targets_tensor[0][0]}\")\n",
    "print(f\"- Negative targets: {targets_tensor[0][1]}\")\n",
    "\n",
    "print(\"\\\\nðŸ’¾ Saving tensors...\")\n",
    "torch.save({\n",
    "    'query_embeddings': query_embeddings_tensor,\n",
    "    'positive_embeddings': positive_embeddings_tensor, \n",
    "    'negative_embeddings': negative_embeddings_tensor,\n",
    "    'targets': targets_tensor,\n",
    "    'embedding_dim': query_embeddings_tensor.shape[1]\n",
    "}, '../data/embedded_training_data.pt')\n",
    "\n",
    "print(\"âœ… Saved to '../data/embedded_training_data.pt'\")\n",
    "print(\"\\\\nðŸš€ Ready for Two Tower model training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
